---
description: Close out a sprint/session with validation, documentation, git operations, and roadmap updates
---

# Sprint/Session Closeout Process

**Purpose**: Validate sprint completion, document accomplishments, perform git operations, and prepare for next sprint.

**Process Flow**:
1. ‚úÖ Validate 80% test coverage with 100% pass rate (BLOCKING)
2. ‚úÖ Verify all session plan items completed (BLOCKING)
3. ‚úÖ Validate modularity architecture compliance (BLOCKING)
4. üìù Perform comprehensive git operations
5. üìÑ Update session documentation
6. üìä Create sprint summary
7. üó∫Ô∏è Update roadmap in CLAUDE.md
8. ‚úì Create verification document
9. üîÑ Prepare retrospective template
10. üìà Generate metrics dashboard
11. üéØ Display closeout summary and next steps

---

## Phase 1: Test Coverage & Pass Rate Validation (BLOCKING)

**Purpose**: Ensure all tests pass with minimum 80% code coverage before allowing closeout.

**Status**: üîç Validating test coverage and pass rate...

### Step 1.1: Run Test Suite with Coverage

```bash
# Run pytest with coverage reporting
echo "Running test suite with coverage analysis..."
pytest --cov=src --cov-report=term-missing --cov-report=json --cov-fail-under=80 -v

# Capture exit code
TEST_EXIT_CODE=$?
```

### Step 1.2: Validate Test Pass Rate

```python
# Check if all tests passed
if TEST_EXIT_CODE != 0:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: Tests Not Passing")
    print("="*80)
    print("\nSprint/session CANNOT be closed until all tests pass.")
    print("\nFailing Tests Summary:")
    print("Run 'pytest -v' to see detailed failure information")
    print("\nNext Steps:")
    print("1. Fix all failing tests")
    print("2. Run 'pytest -v' to verify fixes")
    print("3. Re-run '/end' command after all tests pass")
    print("\n" + "="*80)
    EXIT_COMMAND()
```

### Step 1.3: Validate Code Coverage

```python
import json

# Load coverage data
with open('coverage.json', 'r') as f:
    coverage_data = json.load(f)

total_coverage = coverage_data['totals']['percent_covered']

print(f"\nüìä Code Coverage Report:")
print("="*80)
print(f"Total Coverage: {total_coverage:.2f}%")
print(f"Target Coverage: 80.00%")
print(f"Status: {'‚úÖ PASS' if total_coverage >= 80.0 else '‚ùå FAIL'}")

# Detailed coverage by file
print("\nCoverage by Module:")
for file_path, file_data in coverage_data['files'].items():
    file_coverage = file_data['summary']['percent_covered']
    status_icon = "‚úÖ" if file_coverage >= 80.0 else "‚ö†Ô∏è"
    print(f"  {status_icon} {file_path}: {file_coverage:.1f}%")

print("="*80)

if total_coverage < 80.0:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: Insufficient Code Coverage")
    print("="*80)
    print(f"\nCurrent Coverage: {total_coverage:.2f}%")
    print(f"Required Coverage: 80.00%")
    print(f"Gap: {80.0 - total_coverage:.2f}%")
    print("\nSprint/session CANNOT be closed until coverage meets minimum threshold.")
    print("\nNext Steps:")
    print("1. Run 'pytest --cov=src --cov-report=term-missing' to see uncovered lines")
    print("2. Add tests for uncovered code")
    print("3. Focus on critical paths and new features")
    print("4. Re-run '/end' command after improving coverage")
    print("\n" + "="*80)
    EXIT_COMMAND()

print("\n‚úÖ Test validation PASSED: All tests passing with sufficient coverage")
```

**Status**: ‚úÖ Phase 1 Complete

---

## Phase 1.5: Test Quality Validation (BLOCKING) - ENHANCED

**Purpose**: Validate test suite quality beyond coverage metrics. Session CANNOT close if HIGH severity anti-patterns detected. Calculates a comprehensive test quality score.

**Status**: üîç Validating test quality...

### Step 1.5.1: Comprehensive Test Anti-pattern Detection

```python
import os
import re
from pathlib import Path
from datetime import datetime

# Find all test files (Python)
test_files_py = list(Path("tests").rglob("test_*.py")) + list(Path("tests").rglob("*_test.py"))

# Find all test files (TypeScript/JavaScript)
test_files_ts = list(Path("tests").rglob("*.test.ts")) + list(Path("tests").rglob("*.spec.ts"))
test_files_ts += list(Path("__tests__").rglob("*.test.ts")) + list(Path("__tests__").rglob("*.spec.ts"))

test_files = test_files_py + test_files_ts
language = "python" if test_files_py else "typescript" if test_files_ts else "unknown"

# Enhanced anti-pattern tracking with severity levels
anti_patterns = {
    "sleep_in_tests": {"count": 0, "files": [], "severity": "HIGH"},
    "assertion_free_tests": {"count": 0, "files": [], "severity": "HIGH"},
    "trivial_assertions": {"count": 0, "files": [], "severity": "MEDIUM"},
    "mock_overuse": {"count": 0, "files": [], "severity": "MEDIUM"},
    "interaction_assertions": {"count": 0, "files": [], "severity": "MEDIUM"},
    "poor_test_names": {"count": 0, "files": [], "severity": "LOW"}
}

# Test metrics tracking
test_metrics = {
    "total_test_files": len(test_files),
    "total_tests": 0,
    "total_assertions": 0,
    "tests_with_good_names": 0
}

print("üîç Scanning test files for anti-patterns and calculating quality score...")
print(f"   Language detected: {language}")
print(f"   Test files found: {len(test_files)}")
print()

for test_file in test_files:
    try:
        content = test_file.read_text()
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not read {test_file}: {e}")
        continue

    # Count test functions
    if language == "python":
        test_functions = re.findall(r'def (test_\w+)\([^)]*\):', content)
    else:  # TypeScript
        test_functions = re.findall(r'(?:it|test)\s*\(\s*[\'"]([^\'"]+)[\'"]', content)

    test_metrics["total_tests"] += len(test_functions)

    # Count assertions
    if language == "python":
        assertion_count = len(re.findall(r'\bassert\b|\bpytest\.raises\b', content))
    else:
        assertion_count = len(re.findall(r'expect\s*\(|assert\w*\s*\(', content))

    test_metrics["total_assertions"] += assertion_count

    # HIGH SEVERITY: Check for sleep in tests
    sleep_matches = re.findall(r'time\.sleep|Thread\.sleep|await asyncio\.sleep|setTimeout.*await', content)
    if sleep_matches:
        anti_patterns["sleep_in_tests"]["count"] += 1
        anti_patterns["sleep_in_tests"]["files"].append(str(test_file))
        print(f"‚ùå HIGH: {test_file}: sleep() detected ({len(sleep_matches)} occurrences)")

    # HIGH SEVERITY: Check for assertion-free tests
    for test_name in test_functions:
        # Extract the test function body
        if language == "python":
            pattern = rf'def {test_name}\([^)]*\):(.*?)(?=\ndef |\nclass |\Z)'
            match = re.search(pattern, content, re.DOTALL)
            if match:
                test_body = match.group(1)
                if 'assert' not in test_body and 'pytest.raises' not in test_body and 'raises(' not in test_body:
                    anti_patterns["assertion_free_tests"]["count"] += 1
                    anti_patterns["assertion_free_tests"]["files"].append(f"{test_file}::{test_name}")
                    print(f"‚ùå HIGH: {test_file}::{test_name}: No assertions found")
        else:
            # TypeScript: Check if test block has expect()
            # Simplified check - look for expect in nearby context
            if f"'{test_name}'" in content or f'"{test_name}"' in content:
                # Find the test block
                test_pattern = rf'(?:it|test)\s*\(\s*[\'\"]{re.escape(test_name)}[\'\"]\s*,.*?\)\s*=>\s*\{{(.*?)\}}\s*\)'
                test_match = re.search(test_pattern, content, re.DOTALL)
                if test_match and 'expect(' not in test_match.group(1):
                    anti_patterns["assertion_free_tests"]["count"] += 1
                    anti_patterns["assertion_free_tests"]["files"].append(f"{test_file}::{test_name}")

    # MEDIUM SEVERITY: Check for trivial assertions
    trivial_patterns = [
        r'assert True\b',
        r'assert 1 == 1\b',
        r'assert not False\b',
        r'expect\(true\)\.toBe\(true\)',
        r'expect\(1\)\.toBe\(1\)',
        r'assert\(\s*true\s*\)'
    ]
    for pattern in trivial_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            anti_patterns["trivial_assertions"]["count"] += 1
            anti_patterns["trivial_assertions"]["files"].append(str(test_file))
            print(f"‚ö†Ô∏è  MEDIUM: {test_file}: Trivial assertion detected")
            break

    # MEDIUM SEVERITY: Check for excessive mocking (>5 Mock() calls in one file)
    if language == "python":
        mock_count = len(re.findall(r'Mock\(|MagicMock\(|patch\(|@patch', content))
    else:
        mock_count = len(re.findall(r'jest\.mock\(|jest\.fn\(|vi\.mock\(|vi\.fn\(', content))

    if mock_count > 5:
        anti_patterns["mock_overuse"]["count"] += 1
        anti_patterns["mock_overuse"]["files"].append(f"{test_file}: {mock_count} mocks")
        print(f"‚ö†Ô∏è  MEDIUM: {test_file}: Excessive mocking ({mock_count} mocks)")

    # MEDIUM SEVERITY: Check for interaction assertions in non-boundary tests
    if "assert_called" in content or "toHaveBeenCalled" in content or "verify(" in content:
        if "gateway" not in str(test_file).lower() and "adapter" not in str(test_file).lower():
            if "integration" not in str(test_file).lower() and "e2e" not in str(test_file).lower():
                anti_patterns["interaction_assertions"]["count"] += 1
                anti_patterns["interaction_assertions"]["files"].append(str(test_file))
                print(f"‚ö†Ô∏è  MEDIUM: {test_file}: Interaction assertions in non-boundary test")

    # LOW SEVERITY: Check for poor test names
    poor_name_patterns = [r'def test_\d+\(', r'def test_it\(', r'def test_stuff\(', r'def test_test\(']
    if language == "python":
        for pattern in poor_name_patterns:
            matches = re.findall(pattern, content)
            if matches:
                anti_patterns["poor_test_names"]["count"] += len(matches)
                anti_patterns["poor_test_names"]["files"].append(f"{test_file}: {matches}")
                print(f"‚ÑπÔ∏è  LOW: {test_file}: Poor test name(s): {matches}")
    else:
        # TypeScript poor names
        poor_ts_patterns = [r"(?:it|test)\s*\(['\"]test\s*\d+['\"]", r"(?:it|test)\s*\(['\"]it works['\"]"]
        for pattern in poor_ts_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                anti_patterns["poor_test_names"]["count"] += 1
                anti_patterns["poor_test_names"]["files"].append(str(test_file))
```

### Step 1.5.2: Calculate Test Quality Score

```python
def calculate_test_quality_score():
    """
    Calculate comprehensive test quality score (0-100).

    Score breakdown:
    - Coverage contribution: 30 pts max (from Phase 1)
    - Assertion density: 15 pts max (2+ assertions per test = full points)
    - Test isolation: 15 pts max (no shared state issues)
    - No HIGH severity issues: 40 pts (20 pts each for sleep and assertion-free)
    - Deductions for MEDIUM/LOW issues
    """
    score = 100

    # Deduct for HIGH severity issues (BLOCKING)
    sleep_deduction = anti_patterns["sleep_in_tests"]["count"] * 20
    assertion_free_deduction = anti_patterns["assertion_free_tests"]["count"] * 10
    high_severity_deduction = min(40, sleep_deduction + assertion_free_deduction)

    # Deduct for MEDIUM severity issues
    trivial_deduction = anti_patterns["trivial_assertions"]["count"] * 5
    mock_deduction = anti_patterns["mock_overuse"]["count"] * 3
    interaction_deduction = anti_patterns["interaction_assertions"]["count"] * 3
    medium_severity_deduction = min(30, trivial_deduction + mock_deduction + interaction_deduction)

    # Deduct for LOW severity issues
    poor_name_deduction = anti_patterns["poor_test_names"]["count"] * 2
    low_severity_deduction = min(10, poor_name_deduction)

    # Calculate assertion density bonus/penalty
    assertion_density = 0
    if test_metrics["total_tests"] > 0:
        assertion_density = test_metrics["total_assertions"] / test_metrics["total_tests"]

    assertion_bonus = 0
    if assertion_density >= 2.0:
        assertion_bonus = 10
    elif assertion_density >= 1.5:
        assertion_bonus = 5
    elif assertion_density < 1.0:
        assertion_bonus = -10

    # Final score calculation
    score = score - high_severity_deduction - medium_severity_deduction - low_severity_deduction + assertion_bonus
    score = max(0, min(100, score))

    return {
        "score": score,
        "high_deduction": high_severity_deduction,
        "medium_deduction": medium_severity_deduction,
        "low_deduction": low_severity_deduction,
        "assertion_bonus": assertion_bonus,
        "assertion_density": assertion_density
    }

def score_to_grade(score):
    if score >= 90: return "A"
    if score >= 80: return "B"
    if score >= 70: return "C"
    if score >= 60: return "D"
    return "F"

# Calculate score
quality_result = calculate_test_quality_score()
test_quality_score = quality_result["score"]
test_quality_grade = score_to_grade(test_quality_score)

print("\n" + "="*80)
print("üìä TEST QUALITY REPORT")
print("="*80)

print(f"\nüéØ Overall Score: {test_quality_score}/100 (Grade {test_quality_grade})")
print(f"   Tests Analyzed: {test_metrics['total_tests']} across {test_metrics['total_test_files']} files")
print(f"   Assertion Density: {quality_result['assertion_density']:.2f} per test (target: 2.0+)")

print("\nüìã Score Breakdown:")
print(f"   Base Score:                    100")
print(f"   HIGH Severity Deductions:      -{quality_result['high_deduction']}")
print(f"   MEDIUM Severity Deductions:    -{quality_result['medium_deduction']}")
print(f"   LOW Severity Deductions:       -{quality_result['low_deduction']}")
print(f"   Assertion Density Bonus:       {'+' if quality_result['assertion_bonus'] >= 0 else ''}{quality_result['assertion_bonus']}")
print(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
print(f"   Final Score:                   {test_quality_score}")

print("\nüìã Anti-Pattern Summary:")
print(f"   ‚ùå HIGH: sleep() in tests:           {anti_patterns['sleep_in_tests']['count']} files")
print(f"   ‚ùå HIGH: Assertion-free tests:       {anti_patterns['assertion_free_tests']['count']} tests")
print(f"   ‚ö†Ô∏è  MEDIUM: Trivial assertions:       {anti_patterns['trivial_assertions']['count']} files")
print(f"   ‚ö†Ô∏è  MEDIUM: Mock overuse (>5/file):   {anti_patterns['mock_overuse']['count']} files")
print(f"   ‚ö†Ô∏è  MEDIUM: Interaction assertions:   {anti_patterns['interaction_assertions']['count']} files")
print(f"   ‚ÑπÔ∏è  LOW: Poor test names:             {anti_patterns['poor_test_names']['count']} tests")
print("="*80)
```

### Step 1.5.3: Block on HIGH Severity Issues

```python
# BLOCKING: sleep in tests is a hard failure
if anti_patterns["sleep_in_tests"]["count"] > 0:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: sleep() in Tests")
    print("="*80)
    print("\nTests with sleep() create flaky, slow test suites.")
    print("\nAffected files:")
    for f in anti_patterns["sleep_in_tests"]["files"][:5]:
        print(f"  ‚ùå {f}")
    if len(anti_patterns["sleep_in_tests"]["files"]) > 5:
        print(f"  ... and {len(anti_patterns['sleep_in_tests']['files']) - 5} more")
    print("\nREQUIRED FIX:")
    print("  Python: Use tenacity, polling, or pytest-timeout")
    print("  JS: Use waitFor from testing-library or vi.waitFor")
    print("\nExample fix (Python):")
    print("  # ‚ùå BAD")
    print("  time.sleep(5)")
    print("  assert result.is_ready")
    print("")
    print("  # ‚úÖ GOOD")
    print("  from tenacity import retry, stop_after_delay")
    print("  @retry(stop=stop_after_delay(5))")
    print("  def wait_for_ready():")
    print("      assert result.is_ready")
    print("\n" + "="*80)
    EXIT_COMMAND()

# BLOCKING: Assertion-free tests verify nothing
if anti_patterns["assertion_free_tests"]["count"] > 0:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: Tests Without Assertions")
    print("="*80)
    print("\nTests without assertions verify nothing and give false confidence.")
    print(f"\n{anti_patterns['assertion_free_tests']['count']} tests found with no assertions:")
    for t in anti_patterns["assertion_free_tests"]["files"][:10]:
        print(f"  ‚ùå {t}")
    if len(anti_patterns["assertion_free_tests"]["files"]) > 10:
        print(f"  ... and {len(anti_patterns['assertion_free_tests']['files']) - 10} more")
    print("\nREQUIRED FIX:")
    print("  Add meaningful assertions that verify expected behavior")
    print("\nExample:")
    print("  # ‚ùå BAD - no assertions")
    print("  def test_user_creation():")
    print("      user = create_user('test@example.com')")
    print("")
    print("  # ‚úÖ GOOD - verifies behavior")
    print("  def test_user_creation():")
    print("      user = create_user('test@example.com')")
    print("      assert user.email == 'test@example.com'")
    print("      assert user.id is not None")
    print("\n" + "="*80)
    EXIT_COMMAND()

# Store quality metrics for sprint summary (Phase 6)
test_quality_metrics = {
    "score": test_quality_score,
    "grade": test_quality_grade,
    "total_tests": test_metrics["total_tests"],
    "total_files": test_metrics["total_test_files"],
    "assertion_density": quality_result["assertion_density"],
    "anti_patterns": {
        "sleep_in_tests": anti_patterns["sleep_in_tests"]["count"],
        "assertion_free_tests": anti_patterns["assertion_free_tests"]["count"],
        "trivial_assertions": anti_patterns["trivial_assertions"]["count"],
        "mock_overuse": anti_patterns["mock_overuse"]["count"],
        "poor_test_names": anti_patterns["poor_test_names"]["count"]
    }
}

print(f"\n‚úÖ Test quality validation PASSED (Score: {test_quality_score}/100, Grade: {test_quality_grade})")

# Generate improvement suggestions
suggestions = []
if anti_patterns["trivial_assertions"]["count"] > 0:
    suggestions.append("Replace trivial assertions (assert True) with meaningful behavior checks")
if anti_patterns["mock_overuse"]["count"] > 0:
    suggestions.append("Consider integration tests instead of heavy mocking")
if anti_patterns["poor_test_names"]["count"] > 0:
    suggestions.append("Use descriptive test names: test_<scenario>_<expected_result>")
if quality_result["assertion_density"] < 2.0:
    suggestions.append(f"Increase assertion density (currently {quality_result['assertion_density']:.1f}, target: 2.0+)")

if suggestions:
    print("\nüí° Suggestions for Improvement:")
    for i, suggestion in enumerate(suggestions, 1):
        print(f"   {i}. {suggestion}")
```

### Step 1.5.2: Validate Module Boundaries (BLOCKING)

```python
import subprocess

print("Validating module boundaries...")

# Python: Run import-linter
if os.path.exists("pyproject.toml"):
    # Check if import-linter config exists
    with open("pyproject.toml", "r") as f:
        if "[tool.importlinter]" in f.read():
            result = subprocess.run(
                ["lint-imports"],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                print("\n" + "="*80)
                print("‚ùå BLOCKING VALIDATION FAILED: Architectural Violations")
                print("="*80)
                print(result.stdout)
                print("\nModule boundary violations detected:")
                print("- Layer dependencies violated (domain importing infrastructure)")
                print("- OR circular dependencies between modules")
                print("\nFix all violations before closing session.")
                print("="*80)
                EXIT_COMMAND()
            print("‚úÖ Python module boundaries validated")

# JavaScript: Run dependency-cruiser
if os.path.exists("package.json") and os.path.exists(".dependency-cruiser.js"):
    result = subprocess.run(
        ["npx", "dependency-cruiser", "src", "--config", ".dependency-cruiser.js"],
        capture_output=True,
        text=True
    )
    if result.returncode != 0:
        print("\n" + "="*80)
        print("‚ùå BLOCKING VALIDATION FAILED: Architectural Violations")
        print("="*80)
        print(result.stdout)
        print("\nFix all violations before closing session.")
        print("="*80)
        EXIT_COMMAND()
    print("‚úÖ JavaScript module boundaries validated")

print("‚úÖ Module boundaries validated - no violations")
```

### Step 1.5.3: Contract Test Validation (BLOCKING if contracts exist)

```python
import glob

pact_files = glob.glob("pacts/*.json")

if pact_files:
    print(f"Found {len(pact_files)} consumer contracts...")

    # Verify provider meets all contracts
    result = subprocess.run(
        ["pact", "verify", "--provider-base-url", "http://localhost:8000"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        print("\n" + "="*80)
        print("‚ùå BLOCKING VALIDATION FAILED: Contract Violations")
        print("="*80)
        print("\nProvider is NOT meeting consumer contract expectations.")
        print("This will cause runtime failures in consuming modules.\n")
        print(result.stdout)
        print("\nFix contract violations before closing session.")
        print("="*80)
        EXIT_COMMAND()

    print("‚úÖ All consumer contracts satisfied")
else:
    print("‚ÑπÔ∏è  No contract tests found (pacts/ directory empty or not configured)")
```

**Status**: ‚úÖ Phase 1.5 Complete

---

## Phase 1.7: UAT Completion Validation (BLOCKING)

**Purpose**: Validate that User Acceptance Testing has been completed for features with frontend components. Session CANNOT close if critical UAT scenarios have failed or if API host violations were detected.

**Status**: üîç Validating UAT completion...

### Step 1.7.1: Detect Features Requiring UAT

```python
import os
import json
import glob
from datetime import datetime

print("\n" + "="*80)
print("üß™ UAT COMPLETION VALIDATION")
print("="*80)

# Find all feature specs with UAT plans
features_with_uat = []
spec_dirs = glob.glob(".specify/specs/*/")

for spec_dir in spec_dirs:
    feature_name = os.path.basename(spec_dir.rstrip('/'))
    uat_plan_path = f"{spec_dir}uat-plan.md"
    uat_results_dir = f"{spec_dir}uat-results/"

    if os.path.exists(uat_plan_path):
        features_with_uat.append({
            "name": feature_name,
            "uat_plan": uat_plan_path,
            "results_dir": uat_results_dir,
            "has_results": os.path.isdir(uat_results_dir)
        })

if not features_with_uat:
    print("\n‚ÑπÔ∏è  No UAT plans found - skipping UAT validation")
    print("   (UAT plans are generated by /project:uat)")
    uat_validation_result = {"skipped": True, "reason": "No UAT plans found"}
else:
    print(f"\nFound {len(features_with_uat)} feature(s) with UAT plans:")
    for f in features_with_uat:
        status = "‚úì has results" if f["has_results"] else "‚ö† no results"
        print(f"   - {f['name']} ({status})")
```

### Step 1.7.2: Validate UAT Results Exist

```python
features_missing_uat = []

for feature in features_with_uat:
    if not feature["has_results"]:
        features_missing_uat.append(feature["name"])
        continue

    # Check for recent UAT results
    results_dir = feature["results_dir"]
    json_results = glob.glob(f"{results_dir}uat_*.json")

    if not json_results:
        features_missing_uat.append(feature["name"])

if features_missing_uat:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: UAT Results Missing")
    print("="*80)
    print("\nThe following features have UAT plans but no test results:")
    for f in features_missing_uat:
        print(f"   ‚ùå {f}")
    print("\nSession CANNOT close until UAT tests are executed.")
    print("\nRun UAT tests:")
    for f in features_missing_uat:
        print(f"   /session:uat {f}")
    print("\n" + "="*80)
    EXIT_COMMAND()

print("\n‚úÖ UAT results exist for all features")
```

### Step 1.7.3: Validate Critical Scenarios Passed

```python
failed_critical_scenarios = []
all_uat_results = []

for feature in features_with_uat:
    results_dir = feature["results_dir"]
    json_results = sorted(glob.glob(f"{results_dir}uat_*.json"), reverse=True)

    if not json_results:
        continue

    # Load most recent results
    latest_result_path = json_results[0]
    try:
        with open(latest_result_path) as f:
            results = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"‚ö†Ô∏è  Could not parse {latest_result_path}: {e}")
        continue

    all_uat_results.append({
        "feature": feature["name"],
        "results": results
    })

    # Check for failed critical scenarios
    # Look for scenarios marked as Critical/Smoke that failed
    scenario_status = results.get("scenario_status", {})
    scenarios = results.get("scenarios", [])

    for scenario in scenarios:
        scenario_id = scenario.get("id", "unknown")
        passed = scenario.get("passed")

        # Check if critical (look for Critical priority or smoke tag)
        is_critical = (
            scenario.get("priority") == "Critical" or
            "smoke" in scenario_id.lower() or
            "critical" in scenario.get("title", "").lower()
        )

        if is_critical and passed is False:
            failed_critical_scenarios.append({
                "feature": feature["name"],
                "scenario": scenario_id,
                "title": scenario.get("title", "Unknown")
            })

if failed_critical_scenarios:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: Critical UAT Scenarios Failed")
    print("="*80)
    print("\nThe following critical UAT scenarios have failed:")
    for f in failed_critical_scenarios:
        print(f"   ‚ùå {f['feature']}/{f['scenario']}: {f['title']}")
    print("\nSession CANNOT close until all critical scenarios pass.")
    print("\nTo fix:")
    print("   1. Review failed scenarios in UAT reports")
    print("   2. Fix the underlying issues in code")
    print("   3. Re-run UAT: /session:uat <feature-name>")
    print("\n" + "="*80)
    EXIT_COMMAND()

print("‚úÖ All critical UAT scenarios passed")
```

### Step 1.7.4: Validate No API Host Violations

```python
api_violations = []

for result_data in all_uat_results:
    feature = result_data["feature"]
    results = result_data["results"]

    # Check for API host violations
    violations = results.get("api_violations", [])
    if violations:
        for v in violations:
            api_violations.append({
                "feature": feature,
                "type": v.get("type", "unknown"),
                "message": v.get("message", ""),
                "expected": v.get("expected", "")
            })

    # Also check scenario-level API validations
    for scenario in results.get("scenarios", []):
        scenario_violations = scenario.get("api_violations", [])
        for v in scenario_violations:
            api_violations.append({
                "feature": feature,
                "scenario": scenario.get("id"),
                "type": v.get("type", "unknown"),
                "message": v.get("message", "")
            })

if api_violations:
    print("\n" + "="*80)
    print("‚ùå BLOCKING VALIDATION FAILED: API Host Violations Detected")
    print("="*80)
    print("\nFrontend code is calling incorrect API hosts!")
    print("This will cause failures in production.\n")
    print("Violations found:")
    for v in api_violations[:10]:  # Show first 10
        print(f"   ‚ùå [{v['feature']}] {v['type']}: {v.get('message', '')[:60]}")
    if len(api_violations) > 10:
        print(f"   ... and {len(api_violations) - 10} more")
    print("\nCommon causes:")
    print("   - Hardcoded localhost URLs in API calls")
    print("   - Missing API_BASE_URL environment variable usage")
    print("   - Hardcoded staging/production URLs")
    print("\nRequired fix:")
    print("   1. Use environment variables for API base URL")
    print("   2. Never hardcode localhost, staging, or production URLs")
    print("   3. Re-run UAT to verify: /session:uat <feature-name>")
    print("\n" + "="*80)
    EXIT_COMMAND()

print("‚úÖ No API host violations detected")
```

### Step 1.7.5: Generate UAT Summary

```python
# Store UAT metrics for sprint summary (Phase 6)
uat_metrics = {
    "features_tested": len(features_with_uat),
    "total_scenarios": sum(
        len(r["results"].get("scenarios", []))
        for r in all_uat_results
    ),
    "passed_scenarios": sum(
        sum(1 for s in r["results"].get("scenarios", []) if s.get("passed"))
        for r in all_uat_results
    ),
    "failed_scenarios": sum(
        sum(1 for s in r["results"].get("scenarios", []) if s.get("passed") is False)
        for r in all_uat_results
    ),
    "api_violations": len(api_violations),
    "validation_passed": True
}

print(f"\nüìä UAT Summary:")
print(f"   Features tested: {uat_metrics['features_tested']}")
print(f"   Scenarios: {uat_metrics['total_scenarios']} total")
print(f"   ‚úÖ Passed: {uat_metrics['passed_scenarios']}")
print(f"   ‚ùå Failed: {uat_metrics['failed_scenarios']}")
print(f"   API Violations: {uat_metrics['api_violations']}")

print("\n‚úÖ UAT validation PASSED")
```

**Status**: ‚úÖ Phase 1.7 Complete

---

## Phase 1.75: Code Quality Self-Reflection (BLOCKING)

**Purpose**: Perform LLM-based self-evaluation of code written during this session to determine if a second pass would result in better code. This is a blocking validation - if quality is poor (Grade D or F), the session cannot end until improvements are made.

**Status**: üîç Evaluating code quality...

### Step 1.75.1: Gather Session Changes

```python
import subprocess
import os
import json
from datetime import datetime

print("\n" + "="*80)
print("üîç CODE QUALITY SELF-REFLECTION")
print("="*80)

# Get list of files changed in this session
# Check if we're in a git repository
result = subprocess.run(
    ["git", "rev-parse", "--is-inside-work-tree"],
    capture_output=True,
    text=True
)

if result.returncode != 0:
    print("‚ö†Ô∏è Not a git repository - skipping code quality self-reflection")
    code_quality_metrics = {"skipped": True, "reason": "Not a git repository"}
else:
    # Get the diff of changes (staged and unstaged)
    diff_result = subprocess.run(
        ["git", "diff", "HEAD~1", "--name-only", "--diff-filter=ACMR"],
        capture_output=True,
        text=True
    )

    if diff_result.returncode != 0:
        # Fallback: get all staged and modified files
        diff_result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True
        )
        changed_files = [
            line[3:].strip() for line in diff_result.stdout.strip().split('\n')
            if line.strip() and not line.startswith('?')
        ]
    else:
        changed_files = [f for f in diff_result.stdout.strip().split('\n') if f]

    # Filter to only code files
    code_extensions = {'.py', '.ts', '.tsx', '.js', '.jsx', '.go', '.rs', '.java'}
    code_files = [
        f for f in changed_files
        if any(f.endswith(ext) for ext in code_extensions)
        and not f.startswith('test')
        and '_test.' not in f
        and '.test.' not in f
        and '/tests/' not in f
    ]

    print(f"\nüìÇ Files to evaluate: {len(code_files)}")
    for f in code_files[:10]:  # Show first 10
        print(f"   - {f}")
    if len(code_files) > 10:
        print(f"   ... and {len(code_files) - 10} more")
```

### Step 1.75.2: Perform Self-Reflection Evaluation

```python
# Quality dimensions to evaluate (1-10 scale each)
quality_dimensions = {
    "readability": {
        "score": 0,
        "criteria": "Clear naming, consistent style, logical structure, follows conventions",
        "weight": 1.0
    },
    "maintainability": {
        "score": 0,
        "criteria": "Low coupling, high cohesion, easy to extend, DRY principles",
        "weight": 1.0
    },
    "test_quality": {
        "score": 0,
        "criteria": "Meaningful assertions, edge cases covered, good isolation",
        "weight": 1.0
    },
    "error_handling": {
        "score": 0,
        "criteria": "Specific exceptions, graceful failures, proper logging",
        "weight": 0.8
    },
    "documentation": {
        "score": 0,
        "criteria": "Clear docstrings, type hints, comments where needed",
        "weight": 0.7
    },
    "architecture": {
        "score": 0,
        "criteria": "Appropriate patterns, clean abstractions, single responsibility",
        "weight": 0.9
    },
    "efficiency": {
        "score": 0,
        "criteria": "Optimal algorithms, no unnecessary complexity, appropriate data structures, no premature optimization",
        "weight": 0.9
    }
}

if not code_files:
    print("\n‚úÖ No code files changed - skipping self-reflection")
    code_quality_metrics = {"skipped": True, "reason": "No code files changed"}
else:
    print("\nü§î Performing self-reflection on code quality...")
    print("-" * 60)

    # Read the changed files content for evaluation
    files_content = {}
    total_lines = 0
    for filepath in code_files:
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    files_content[filepath] = content
                    total_lines += len(content.split('\n'))
            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not read {filepath}: {e}")

    print(f"   Total lines of code to evaluate: {total_lines}")

    # Self-reflection prompt (to be evaluated by the LLM)
    # The LLM should honestly assess the code it wrote

    reflection_prompt = f"""
    As the author of this code, perform an honest self-reflection:

    Files changed: {len(code_files)}
    Total lines: {total_lines}

    For each dimension, score 1-10 and note specific improvements:

    1. READABILITY (naming, structure, conventions)
       - Are variable/function names descriptive?
       - Is the code structure logical and easy to follow?
       - Does it follow language conventions?

    2. MAINTAINABILITY (coupling, cohesion, extensibility)
       - Could this code be easily modified later?
       - Are dependencies well-managed?
       - Is there unnecessary duplication?

    3. TEST QUALITY (assertions, edge cases, isolation)
       - Do tests verify actual behavior?
       - Are edge cases covered?
       - Are tests independent?

    4. ERROR HANDLING (exceptions, failures, logging)
       - Are exceptions specific and caught appropriately?
       - Does the code fail gracefully?
       - Is there sufficient logging/debugging info?

    5. DOCUMENTATION (docstrings, types, comments)
       - Are public interfaces documented?
       - Are type hints present where helpful?
       - Are complex sections explained?

    6. ARCHITECTURE (patterns, abstractions, SRP)
       - Are appropriate patterns used?
       - Is complexity managed through good abstractions?
       - Does each component have a single responsibility?

    7. EFFICIENCY (algorithms, complexity, optimization)
       - Are algorithms appropriate for the data size?
       - Are there any O(n¬≤) operations that could be O(n)?
       - Is complexity justified or over-engineered?
       - Are data structures appropriate for access patterns?
       - Is there unnecessary work (repeated calculations, redundant loops)?

    Would a second pass result in significantly better code?
    """

    # Perform self-evaluation
    # NOTE: This is a self-reflection - the LLM evaluates its own work honestly

    # Default to a moderate self-assessment
    # In practice, Claude should override these with actual reflection

    # Evaluate based on observable patterns in the code
    for filepath, content in files_content.items():
        lines = content.split('\n')

        # Check for readability indicators
        has_docstrings = '"""' in content or "'''" in content
        has_type_hints = ': ' in content and '->' in content
        has_long_functions = any(
            len(line) > 120 for line in lines
        )

        # Check for maintainability
        has_global_state = 'global ' in content
        function_count = content.count('def ') + content.count('function ')
        class_count = content.count('class ')

        # Adjust scores based on code patterns
        if has_docstrings:
            quality_dimensions["documentation"]["score"] += 2
        if has_type_hints:
            quality_dimensions["documentation"]["score"] += 1
            quality_dimensions["readability"]["score"] += 1
        if has_global_state:
            quality_dimensions["maintainability"]["score"] -= 2
        if not has_long_functions:
            quality_dimensions["readability"]["score"] += 1

        # Check for efficiency indicators
        # Nested loops (potential O(n¬≤))
        nested_loop_count = 0
        for i, line in enumerate(lines):
            stripped = line.strip()
            if stripped.startswith('for ') or stripped.startswith('while '):
                # Check next 10 lines for nested loop
                for j in range(i+1, min(i+11, len(lines))):
                    inner = lines[j].strip()
                    if inner.startswith('for ') or inner.startswith('while '):
                        nested_loop_count += 1
                        break

        # List comprehensions inside loops (repeated work)
        import re
        list_comp_in_loop = len(re.findall(r'for .+:\s*\n\s+.*\[.+ for .+ in', content))

        # Repeated string concatenation in loops
        string_concat_loop = len(re.findall(r'for .+:[\s\S]{0,100}\+\s*=\s*["\']', content))

        # Using list when set would be better for lookups
        list_in_check = len(re.findall(r'if .+ in \[', content))

        # Adjust efficiency score based on detected patterns
        if nested_loop_count > 2:
            quality_dimensions["efficiency"]["score"] -= 2
        elif nested_loop_count > 0:
            quality_dimensions["efficiency"]["score"] -= 1

        if list_comp_in_loop > 0:
            quality_dimensions["efficiency"]["score"] -= 1

        if string_concat_loop > 0:
            quality_dimensions["efficiency"]["score"] -= 1

        if list_in_check > 2:
            quality_dimensions["efficiency"]["score"] -= 1

        # Positive efficiency indicators
        has_generators = ' yield ' in content or '(x for x' in content
        has_set_lookups = 'set(' in content or ': set' in content
        has_dict_comprehension = '{' in content and ' for ' in content and '}' in content

        if has_generators:
            quality_dimensions["efficiency"]["score"] += 1
        if has_set_lookups:
            quality_dimensions["efficiency"]["score"] += 1
        if has_dict_comprehension:
            quality_dimensions["efficiency"]["score"] += 1

    # Normalize scores to 1-10 range
    # Base score of 6 (acceptable), adjusted by findings
    for dim in quality_dimensions:
        base_score = 6
        adjustment = quality_dimensions[dim]["score"]
        final_score = max(1, min(10, base_score + adjustment))
        quality_dimensions[dim]["score"] = final_score
```

### Step 1.75.3: Calculate Overall Quality Score

```python
def calculate_quality_grade(dimensions):
    """Calculate weighted average and letter grade."""
    total_weight = sum(d["weight"] for d in dimensions.values())
    weighted_sum = sum(
        d["score"] * d["weight"]
        for d in dimensions.values()
    )

    overall_score = weighted_sum / total_weight if total_weight > 0 else 0

    # Determine grade
    if overall_score >= 9:
        grade = "A"
        description = "Excellent - production-ready code"
    elif overall_score >= 7:
        grade = "B"
        description = "Good - minor improvements possible"
    elif overall_score >= 5:
        grade = "C"
        description = "Acceptable - proceed with noted improvements"
    elif overall_score >= 3:
        grade = "D"
        description = "Poor - improvements required before proceeding"
    else:
        grade = "F"
        description = "Failing - significant rework needed"

    return {
        "overall_score": round(overall_score, 1),
        "grade": grade,
        "description": description,
        "blocks_session": grade in ["D", "F"],
        "second_pass_recommended": overall_score < 7
    }

quality_result = calculate_quality_grade(quality_dimensions)

# Display results
print(f"\nüìä CODE QUALITY SELF-REFLECTION RESULTS")
print("=" * 60)
print(f"\n   Overall Score: {quality_result['overall_score']}/10")
print(f"   Grade: {quality_result['grade']} - {quality_result['description']}")
print(f"\n   Dimension Breakdown:")
for dim_name, dim_data in quality_dimensions.items():
    bar = "‚ñà" * dim_data["score"] + "‚ñë" * (10 - dim_data["score"])
    print(f"   {dim_name.replace('_', ' ').title():20} [{bar}] {dim_data['score']}/10")

# Store metrics for sprint summary
code_quality_metrics = {
    "skipped": False,
    "overall_score": quality_result["overall_score"],
    "grade": quality_result["grade"],
    "description": quality_result["description"],
    "dimensions": {
        name: data["score"] for name, data in quality_dimensions.items()
    },
    "files_evaluated": len(code_files),
    "lines_evaluated": total_lines,
    "second_pass_recommended": quality_result["second_pass_recommended"]
}
```

### Step 1.75.4: Block on Poor Quality (BLOCKING)

```python
# Identify specific improvements needed
improvements_needed = []

if quality_dimensions["readability"]["score"] < 5:
    improvements_needed.append({
        "dimension": "Readability",
        "issue": "Code is hard to read or follow",
        "action": "Improve naming, reduce complexity, follow conventions"
    })

if quality_dimensions["maintainability"]["score"] < 5:
    improvements_needed.append({
        "dimension": "Maintainability",
        "issue": "Code will be hard to modify later",
        "action": "Reduce coupling, eliminate duplication, improve cohesion"
    })

if quality_dimensions["test_quality"]["score"] < 5:
    improvements_needed.append({
        "dimension": "Test Quality",
        "issue": "Tests may not catch real issues",
        "action": "Add meaningful assertions, cover edge cases"
    })

if quality_dimensions["error_handling"]["score"] < 4:
    improvements_needed.append({
        "dimension": "Error Handling",
        "issue": "Errors may not be handled gracefully",
        "action": "Add specific exception handling and logging"
    })

if quality_dimensions["architecture"]["score"] < 5:
    improvements_needed.append({
        "dimension": "Architecture",
        "issue": "Design may not scale well",
        "action": "Apply appropriate patterns, improve abstractions"
    })

# Check if session should be blocked
if quality_result["blocks_session"]:
    print("\n" + "=" * 80)
    print("‚ùå BLOCKING VALIDATION FAILED: Code Quality Below Threshold")
    print("=" * 80)
    print(f"\n   Grade {quality_result['grade']} ({quality_result['overall_score']}/10) is below the minimum (C/5.0)")
    print("\n   The code written in this session requires improvement before")
    print("   the session can be closed. This ensures we don't merge poor quality")
    print("   code that will need immediate refactoring.")

    print("\nüìã Required Improvements:")
    for i, improvement in enumerate(improvements_needed, 1):
        print(f"\n   {i}. {improvement['dimension']}")
        print(f"      Issue: {improvement['issue']}")
        print(f"      Action: {improvement['action']}")

    print("\n" + "-" * 60)
    print("   RECOMMENDED ACTIONS:")
    print("   1. Review the files listed above")
    print("   2. Address the identified issues")
    print("   3. Run /session:end again after improvements")
    print("-" * 60)
    print("\n" + "=" * 80)
    EXIT_COMMAND()

# Display suggestions for non-blocking improvements
if improvements_needed and not quality_result["blocks_session"]:
    print("\nüí° Suggested Improvements (Non-Blocking):")
    for improvement in improvements_needed:
        print(f"   ‚Ä¢ {improvement['dimension']}: {improvement['action']}")

if quality_result["second_pass_recommended"]:
    print("\nüìù Note: A second pass on this code could improve quality.")
    print("   Consider reviewing before the next session.")

print(f"\n‚úÖ Code quality self-reflection PASSED (Grade: {quality_result['grade']})")
```

**Status**: ‚úÖ Phase 1.75 Complete

---

## Phase 2: Session Plan Completion Validation (BLOCKING)

**Purpose**: Verify all planned tasks are complete or properly documented as incomplete.

**Status**: üîç Validating session plan completion...

### Step 2.1: Load Session State

```python
import os
import re
from datetime import datetime

# Load current session state
session_state_path = 'state/current_session.md'
if not os.path.exists(session_state_path):
    print("‚ö†Ô∏è Warning: No current session state found")
    print("Continuing with validation using sprint plan only...")
    session_tasks = []
else:
    with open(session_state_path, 'r') as f:
        session_content = f.read()

    # Extract session tasks
    session_tasks = []
    # Parse tasks from session state
    task_pattern = r'- \[([ x])\] (.+)'
    for match in re.finditer(task_pattern, session_content):
        completed = match.group(1) == 'x'
        task_desc = match.group(2)
        session_tasks.append({
            'description': task_desc,
            'completed': completed
        })
```

### Step 2.2: Load Sprint Plan

```python
# Determine current sprint from CLAUDE.md
with open('CLAUDE.md', 'r') as f:
    claude_content = f.read()

# Extract current sprint ID
sprint_match = re.search(r'Current Sprint:\s*S?(\d+)', claude_content, re.IGNORECASE)
if sprint_match:
    sprint_id = sprint_match.group(1).zfill(2)
else:
    print("‚ùå ERROR: Could not determine current sprint from CLAUDE.md")
    EXIT_COMMAND()

# Check for spec-kit format first
import glob as glob_module

spec_kit_tasks = glob_module.glob('.specify/specs/*/tasks.md')
task_format = None
sprint_plan_path = None
sprint_plan_content = None

if spec_kit_tasks:
    # Use spec-kit format
    task_format = "spec-kit"
    if len(spec_kit_tasks) == 1:
        sprint_plan_path = spec_kit_tasks[0]
    else:
        # Multiple features - use current feature from CLAUDE.md or session state
        feature_match = re.search(r'Current Feature:\s*(\S+)', claude_content)
        if feature_match:
            feature_name = feature_match.group(1)
            sprint_plan_path = f'.specify/specs/{feature_name}/tasks.md'
        else:
            # Default to first feature
            sprint_plan_path = spec_kit_tasks[0]

    if not os.path.exists(sprint_plan_path):
        print(f"‚ùå ERROR: Tasks file not found at {sprint_plan_path}")
        EXIT_COMMAND()

    with open(sprint_plan_path, 'r') as f:
        sprint_plan_content = f.read()

    feature_name = sprint_plan_path.split('/')[2]
    print(f"\nüìã Loading Tasks (Spec-Kit): {feature_name}")
    print("="*80)
else:
    # Fallback to legacy format
    task_format = "legacy"
    sprint_plan_path = f'sprints/sprint_{sprint_id}/sprint_plan.md'
    if not os.path.exists(sprint_plan_path):
        print(f"‚ùå ERROR: Sprint plan not found at {sprint_plan_path}")
        print("   Run /project:scrum to generate .specify/specs/{feature}/tasks.md")
        EXIT_COMMAND()

    with open(sprint_plan_path, 'r') as f:
        sprint_plan_content = f.read()

    print(f"\nüìã Loading Sprint Plan (Legacy): Sprint {sprint_id}")
    print("   ‚ö†Ô∏è Consider migrating to spec-kit format with /project:migrate")

# Check beads for task completion status
if os.path.exists('.beads'):
    print("\nüìä Verifying Beads Task Status...")

    import subprocess
    import json

    # Get all issues from beads
    result = subprocess.run(
        ['bd', 'list', '--json'],
        capture_output=True, text=True
    )
    beads_issues = json.loads(result.stdout) if result.stdout else []

    open_issues = [i for i in beads_issues if i['status'] == 'open']
    closed_issues = [i for i in beads_issues if i['status'] == 'closed']

    if open_issues:
        print(f"   ‚ö†Ô∏è {len(open_issues)} tasks still open in beads:")
        for issue in open_issues[:5]:  # Show first 5
            print(f"      - {issue['id']}: {issue['title']}")
        if len(open_issues) > 5:
            print(f"      ... and {len(open_issues) - 5} more")
        print("\n   Run 'bd ready' to see tasks ready to work on")
    else:
        print(f"   ‚úÖ All {len(closed_issues)} tasks closed in beads")

    # Run bd doctor to check for issues
    doctor_result = subprocess.run(
        ['bd', 'doctor'],
        capture_output=True, text=True
    )
    if 'warning' in doctor_result.stdout.lower() or 'error' in doctor_result.stdout.lower():
        print("   ‚ö†Ô∏è Beads doctor found issues - run 'bd doctor' for details")
    else:
        print("   ‚úÖ Beads health check passed")

    # Sync before closeout
    subprocess.run(['bd', 'sync'])
    print("="*80)
```

### Step 2.3: Parse Tasks/Stories

```python
stories = []
current_story = None

if task_format == "spec-kit":
    # Parse spec-kit tasks.md format
    for line in sprint_plan_content.split('\n'):
        # Match task headers: ### T-XXX: Title
        task_match = re.match(r'^###\s+(T-\d+):\s+(.+)', line)
        if task_match:
            if current_story:
                stories.append(current_story)
            current_story = {
                'id': task_match.group(1),
                'title': task_match.group(2).strip(),
                'points': 0,  # Calculated from hours
                'acceptance_criteria': [],
                'status': 'unknown'
            }

        # Parse status from - **Status**: line
        elif current_story and '**Status**:' in line:
            if 'completed' in line.lower():
                current_story['status'] = 'complete'
            elif 'in_progress' in line.lower():
                current_story['status'] = 'in_progress'
            else:
                current_story['status'] = 'incomplete'

        # Parse estimated hours
        elif current_story and '**Estimated Hours**:' in line:
            hours_match = re.search(r'(\d+\.?\d*)', line)
            if hours_match:
                # Convert hours to points (rough: 2 hours = 1 point)
                current_story['points'] = int(float(hours_match.group(1)) / 2 + 0.5)

        # Match acceptance criteria: - [ ]
        elif current_story and re.match(r'^\s*- \[([ x])\]', line):
            completed = 'x' in line.lower()
            criteria_text = re.sub(r'^\s*- \[([ x])\]\s*', '', line, flags=re.IGNORECASE)
            current_story['acceptance_criteria'].append({
                'text': criteria_text,
                'completed': completed
            })
else:
    # Parse legacy sprint_plan.md format
    for line in sprint_plan_content.split('\n'):
        # Match user story headers: ### US-XXX: Title [status] (points)
        story_match = re.match(r'^###\s+(US-\d+):\s+(.+?)\s+[‚è≥‚úÖ‚ùåüîÑ]?\s*(?:\((\d+)\s+pts?\))?', line)
        if story_match:
            if current_story:
                stories.append(current_story)
            current_story = {
                'id': story_match.group(1),
                'title': story_match.group(2).strip(),
                'points': int(story_match.group(3)) if story_match.group(3) else 0,
                'acceptance_criteria': [],
                'status': 'unknown'
            }
            # Check if story is marked complete
            if '‚úÖ' in line:
                current_story['status'] = 'complete'
            elif '‚è≥' in line or 'üîÑ' in line:
                current_story['status'] = 'in_progress'
            elif '‚ùå' in line:
                current_story['status'] = 'incomplete'

        # Match acceptance criteria: - [x] or - [ ]
        elif current_story and re.match(r'^\s*- \[([ x])\]', line):
            completed = 'x' in line
            criteria_text = re.sub(r'^\s*- \[([ x])\]\s*', '', line)
            current_story['acceptance_criteria'].append({
                'text': criteria_text,
                'completed': completed
            })

# Don't forget the last story
if current_story:
    stories.append(current_story)

item_type = "tasks" if task_format == "spec-kit" else "user stories"
print(f"Found {len(stories)} {item_type} in {'tasks.md' if task_format == 'spec-kit' else 'sprint plan'}")
```

### Step 2.4: Analyze Completion Status

```python
# Calculate completion metrics
total_stories = len(stories)
completed_stories = sum(1 for s in stories if s['status'] == 'complete')
in_progress_stories = sum(1 for s in stories if s['status'] == 'in_progress')
incomplete_stories = sum(1 for s in stories if s['status'] == 'incomplete')

total_points = sum(s['points'] for s in stories)
completed_points = sum(s['points'] for s in stories if s['status'] == 'complete')

completion_rate = (completed_points / total_points * 100) if total_points > 0 else 0

print("\nüìä Sprint Completion Analysis:")
print("="*80)
print(f"Total Stories: {total_stories}")
print(f"  ‚úÖ Complete: {completed_stories}")
print(f"  üîÑ In Progress: {in_progress_stories}")
print(f"  ‚ùå Incomplete: {incomplete_stories}")
print(f"\nStory Points:")
print(f"  Planned: {total_points}")
print(f"  Completed: {completed_points}")
print(f"  Completion Rate: {completion_rate:.1f}%")
print("="*80)
```

### Step 2.5: Verify Incomplete Work is Documented

```python
# Check if there's incomplete work
if in_progress_stories > 0 or incomplete_stories > 0:
    print("\n‚ö†Ô∏è WARNING: Incomplete work detected")
    print("\nIncomplete Stories:")

    incomplete_story_list = [s for s in stories if s['status'] in ['in_progress', 'incomplete']]

    for story in incomplete_story_list:
        print(f"\n  {story['id']}: {story['title']}")
        print(f"    Status: {story['status']}")
        print(f"    Points: {story['points']}")
        print(f"    Acceptance Criteria:")

        for criterion in story['acceptance_criteria']:
            status_icon = "‚úÖ" if criterion['completed'] else "‚ùå"
            print(f"      {status_icon} {criterion['text']}")

    # Check if incomplete work is documented
    incomplete_doc_path = f'sprints/sprint_{sprint_id}/incomplete_work.md'

    if not os.path.exists(incomplete_doc_path):
        print("\n" + "="*80)
        print("‚ùå BLOCKING VALIDATION FAILED: Incomplete Work Not Documented")
        print("="*80)
        print(f"\n{len(incomplete_story_list)} stories are incomplete but no documentation found.")
        print("\nSprint/session CANNOT be closed until incomplete work is documented.")
        print("\nNext Steps:")
        print(f"1. Create {incomplete_doc_path}")
        print("2. Document why each story is incomplete")
        print("3. Document what was completed vs. what remains")
        print("4. Re-estimate remaining work")
        print("5. Plan for completion in next sprint")
        print("\nOr:")
        print("1. Complete the remaining stories")
        print("2. Update sprint plan to mark stories complete")
        print("3. Re-run '/end' command")
        print("\n" + "="*80)
        EXIT_COMMAND()
    else:
        print(f"\n‚úÖ Incomplete work documented in {incomplete_doc_path}")

print("\n‚úÖ Session plan validation PASSED")
```

### Step 2.6: Create Verification Document

```python
# Create verification document
verification_path = f'sprints/sprint_{sprint_id}/verification.md'

verification_content = f"""# Sprint {sprint_id} Closeout Verification

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sprint ID**: {sprint_id}
**Closeout Status**: ‚úÖ Verified

---

## Test Validation

**Test Pass Rate**: 100% ‚úÖ
**Code Coverage**: {total_coverage:.2f}% ‚úÖ
**Coverage Target**: 80.00%
**Status**: PASSED

All tests are passing and code coverage meets minimum threshold.

---

## Sprint Completion

**Total Stories**: {total_stories}
**Completed Stories**: {completed_stories}
**Completion Rate**: {completion_rate:.1f}%

**Story Points**:
- Planned: {total_points}
- Completed: {completed_points}
- Completion: {completion_rate:.1f}%

### Story Status

"""

for story in stories:
    status_icon = {
        'complete': '‚úÖ',
        'in_progress': 'üîÑ',
        'incomplete': '‚ùå',
        'unknown': '‚ö†Ô∏è'
    }.get(story['status'], '‚ö†Ô∏è')

    verification_content += f"\n#### {story['id']}: {story['title']} {status_icon}\n"
    verification_content += f"**Points**: {story['points']}\n"
    verification_content += f"**Status**: {story['status']}\n"

    if story['acceptance_criteria']:
        verification_content += "\n**Acceptance Criteria**:\n"
        for criterion in story['acceptance_criteria']:
            status_icon = "‚úÖ" if criterion['completed'] else "‚ùå"
            verification_content += f"- [{status_icon}] {criterion['text']}\n"

verification_content += f"""

---

## Definition of Done

All completed stories meet the Definition of Done:
- [‚úÖ] Code written and reviewed
- [‚úÖ] Unit tests written and passing
- [‚úÖ] Integration tests passing
- [‚úÖ] Code merged to main branch
- [‚úÖ] Documentation updated
- [‚úÖ] Acceptance criteria validated
- [‚úÖ] No known defects

---

## Quality Metrics

**Code Quality**: ‚úÖ PASSED
- Static analysis: PASSED
- Linting: PASSED
- No critical issues

**Test Quality**: ‚úÖ PASSED
- All tests passing
- Coverage >= 80%

---

## Sign-off

**Verified By**: Claude Code
**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Status**: ‚úÖ Sprint closeout approved

Sprint {sprint_id} has been validated and is approved for closeout.
"""

# Write verification document
with open(verification_path, 'w') as f:
    f.write(verification_content)

print(f"\n‚úÖ Verification document created: {verification_path}")
```

**Status**: ‚úÖ Phase 2 Complete

---

## Phase 3: Modularity Architecture Validation (BLOCKING)

**Purpose**: Ensure code changes maintain modular architecture standards established in Rule 7.

**Status**: üîç Validating modularity compliance...

### Step 3.1: Check for Circular Dependencies

```bash
echo "Checking for circular dependencies..."

CIRCULAR_VIOLATIONS=false

# JavaScript/TypeScript projects
if [ -f "package.json" ]; then
    if command -v npx &> /dev/null; then
        echo "Checking JavaScript/TypeScript circular dependencies..."
        CIRCULAR_CHECK=$(npx madge --circular src/ 2>/dev/null || true)
        if [ -n "$CIRCULAR_CHECK" ] && [ "$CIRCULAR_CHECK" != "No circular dependency found!" ]; then
            echo "‚ùå CIRCULAR DEPENDENCIES DETECTED:"
            echo "$CIRCULAR_CHECK"
            CIRCULAR_VIOLATIONS=true
        else
            echo "‚úÖ No circular dependencies found in JavaScript/TypeScript"
        fi
    else
        echo "‚ö†Ô∏è  npx not available - install madge for circular dependency checking"
    fi
fi

# Python projects
if [ -f "pyproject.toml" ] || [ -f "setup.py" ] || [ -f "requirements.txt" ]; then
    if command -v pydeps &> /dev/null; then
        echo "Checking Python circular dependencies..."
        PYTHON_CIRCULAR=$(pydeps --check-circular src/ 2>/dev/null || true)
        if [ -n "$PYTHON_CIRCULAR" ]; then
            echo "‚ùå CIRCULAR DEPENDENCIES DETECTED IN PYTHON:"
            echo "$PYTHON_CIRCULAR"
            CIRCULAR_VIOLATIONS=true
        else
            echo "‚úÖ No circular dependencies found in Python"
        fi
    else
        echo "‚ö†Ô∏è  pydeps not available - manual review recommended"
    fi
fi

if [ "$CIRCULAR_VIOLATIONS" = true ]; then
    echo ""
    echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
    echo "‚ùå BLOCKING VALIDATION FAILED: Circular Dependencies"
    echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
    echo ""
    echo "Session CANNOT be closed until circular dependencies are resolved."
    echo ""
    echo "Fix: Refactor to break dependency cycles. Common solutions:"
    echo "  - Extract shared interfaces to a common module"
    echo "  - Use dependency injection"
    echo "  - Invert dependencies (depend on abstractions)"
    echo ""
    exit 1
fi

echo "‚úÖ Circular dependency check passed"
```

### Step 3.2: Validate Module Boundaries (No Deep Imports)

```python
import os
import re
import sys

def check_import_boundaries():
    """Check that modules only import from each other's public APIs."""
    violations = []

    # Get all source directories
    src_dirs = []
    for candidate in ['src', 'app', 'lib']:
        if os.path.isdir(candidate):
            src_dirs.append(candidate)

    if not src_dirs:
        print("‚ö†Ô∏è  No standard source directory found (src/, app/, lib/)")
        return []

    for src_dir in src_dirs:
        for root, dirs, files in os.walk(src_dir):
            for file in files:
                if file.endswith('.py'):
                    filepath = os.path.join(root, file)
                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

                    # Check for imports of internal/private modules
                    # Pattern: importing from another module's internal folders
                    internal_patterns = [
                        r'from\s+(\w+)\.(internal|_\w+|private)\.',
                        r'from\s+(\w+)\.(\w+)\.(internal|_\w+|private)\.',
                    ]

                    for pattern in internal_patterns:
                        matches = re.findall(pattern, content)
                        for match in matches:
                            # Get current module
                            path_parts = root.split(os.sep)
                            current_module = path_parts[1] if len(path_parts) > 1 else ''
                            imported_module = match[0]

                            if current_module != imported_module:
                                violations.append({
                                    'file': filepath,
                                    'violation': f'Imports internal module from {imported_module}'
                                })

                elif file.endswith(('.ts', '.tsx', '.js', '.jsx')):
                    filepath = os.path.join(root, file)
                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

                    # Check for deep imports (not from index)
                    deep_import_pattern = r"from\s+['\"](@?\w+)/(?!index)[^'\"]+/[^'\"]+['\"]"
                    matches = re.findall(deep_import_pattern, content)

                    for match in matches:
                        violations.append({
                            'file': filepath,
                            'violation': f'Deep import bypassing public API: {match}'
                        })

    return violations

violations = check_import_boundaries()

if violations:
    print("")
    print("‚ïê" * 64)
    print("‚ùå BLOCKING VALIDATION FAILED: Module Boundary Violations")
    print("‚ïê" * 64)
    print("")
    for v in violations:
        print(f"  File: {v['file']}")
        print(f"  Issue: {v['violation']}")
        print()
    print("Session CANNOT be closed until import boundaries are respected.")
    print("")
    print("Fix: Import only from module's public API:")
    print("  - Python: Import from __init__.py exports")
    print("  - TypeScript: Import from index.ts/index.js")
    print("")
    sys.exit(1)
else:
    print("‚úÖ Module boundaries respected - no internal imports across modules")
```

### Step 3.3: Validate Directory Structure (Feature-Based)

```python
import os
import sys

def check_directory_structure():
    """Validate feature-based (not layer-based) directory organization."""
    errors = []
    warnings = []

    # Anti-pattern: layer-based folders at top level
    layer_folders = [
        'controllers', 'services', 'repositories', 'models',
        'utils', 'helpers', 'common', 'shared'
    ]

    # Check in likely source directories
    for src_dir in ['src', 'app', 'lib']:
        if os.path.isdir(src_dir):
            for folder in layer_folders:
                layer_path = os.path.join(src_dir, folder)
                if os.path.isdir(layer_path):
                    # Count files to determine if it's substantial
                    file_count = sum(1 for _, _, files in os.walk(layer_path) for f in files)
                    if file_count > 3:  # More than 3 files = substantial
                        errors.append(f"Layer-based folder detected: {layer_path}/ ({file_count} files)")
                    else:
                        warnings.append(f"Small layer folder exists: {layer_path}/ ({file_count} files) - consider refactoring")

    # Check that feature folders have proper public API
    for src_dir in ['src', 'app', 'lib']:
        if os.path.isdir(src_dir):
            for item in os.listdir(src_dir):
                feature_path = os.path.join(src_dir, item)
                if os.path.isdir(feature_path) and not item.startswith(('_', '.')):
                    # Skip known non-feature folders
                    if item in ['tests', 'test', '__pycache__', 'node_modules', 'types', 'config']:
                        continue

                    # Check for public API definition
                    has_python_api = os.path.exists(os.path.join(feature_path, '__init__.py'))
                    has_ts_api = os.path.exists(os.path.join(feature_path, 'index.ts'))
                    has_js_api = os.path.exists(os.path.join(feature_path, 'index.js'))

                    if not (has_python_api or has_ts_api or has_js_api):
                        warnings.append(f"Module '{item}' missing public API file (__init__.py or index.ts/js)")

    return errors, warnings

errors, warnings = check_directory_structure()

if errors:
    print("")
    print("‚ïê" * 64)
    print("‚ùå BLOCKING VALIDATION FAILED: Directory Structure Violations")
    print("‚ïê" * 64)
    print("")
    for e in errors:
        print(f"  ‚ùå {e}")
    print("")
    print("Session CANNOT be closed with layer-based organization.")
    print("")
    print("Fix: Refactor to feature-based structure:")
    print("  ‚úÖ src/auth/          (feature: authentication)")
    print("  ‚úÖ src/billing/       (feature: billing)")
    print("  ‚úÖ src/products/      (feature: products)")
    print("  ‚ùå src/controllers/   (layer - anti-pattern)")
    print("  ‚ùå src/services/      (layer - anti-pattern)")
    print("")
    sys.exit(1)

if warnings:
    print("")
    print("‚ö†Ô∏è  MODULARITY WARNINGS (non-blocking):")
    for w in warnings:
        print(f"  ‚ö†Ô∏è  {w}")
    print("(Warnings should be addressed but do not block closeout)")
    print("")

print("‚úÖ Directory structure follows feature-based organization")
```

### Step 3.4: Validate Stable Dependencies Principle

```python
import os
import re
import sys

def check_stable_dependencies():
    """Verify dependencies flow from unstable to stable modules."""

    # Try to read MODULE BOUNDARIES from CLAUDE.md
    module_stability = {}
    claude_md_paths = ['CLAUDE.md', '.claude/CLAUDE.md', 'docs/CLAUDE.md']

    for path in claude_md_paths:
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Look for MODULE BOUNDARIES table
            if '## 8. MODULE BOUNDARIES' in content or '## MODULE BOUNDARIES' in content:
                # Parse stability values from table
                # Format: | module | owner | api | deps | 0.X (Stability) |
                table_pattern = r'\|\s*(\w+)\s*\|[^|]+\|[^|]+\|[^|]+\|\s*([\d.]+)'
                matches = re.findall(table_pattern, content)
                for module, stability in matches:
                    try:
                        module_stability[module.lower()] = float(stability)
                    except ValueError:
                        pass
            break

    if not module_stability:
        print("‚ö†Ô∏è  No MODULE BOUNDARIES table found in CLAUDE.md")
        print("   Skipping Stable Dependencies Principle check")
        print("   Recommendation: Add Section 8 MODULE BOUNDARIES to CLAUDE.md")
        return []

    print(f"Found {len(module_stability)} modules with stability ratings")

    violations = []
    # Analysis would require import graph parsing
    # For now, just validate the table exists and has proper values

    for module, stability in module_stability.items():
        if stability < 0 or stability > 1:
            violations.append(f"Invalid stability value for {module}: {stability} (must be 0-1)")

    return violations

violations = check_stable_dependencies()

if violations:
    print("")
    print("‚ïê" * 64)
    print("‚ùå BLOCKING VALIDATION FAILED: Stable Dependencies Violation")
    print("‚ïê" * 64)
    for v in violations:
        print(f"  ‚ùå {v}")
    print("")
    print("Dependencies must flow from unstable ‚Üí stable modules")
    sys.exit(1)
else:
    print("‚úÖ Stable Dependencies Principle check passed")
```

### Step 3.5: Generate Modularity Report

```python
from datetime import datetime

modularity_report = f"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  MODULARITY VALIDATION RESULTS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**Validation Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Status**: ‚úÖ PASSED

### Checks Performed

| Check | Status | Details |
|-------|--------|---------|
| Circular Dependencies | ‚úÖ Pass | No cycles detected |
| Module Boundaries | ‚úÖ Pass | All imports use public APIs |
| Directory Structure | ‚úÖ Pass | Feature-based organization |
| Stable Dependencies | ‚úÖ Pass | Dependencies flow correctly |

### Modularity Principles Validated

1. ‚úÖ Package by Feature (not layer-based structure)
2. ‚úÖ Acyclic Dependencies (no circular imports)
3. ‚úÖ Information Hiding (public APIs respected)
4. ‚úÖ Stable Dependencies Principle (if MODULE BOUNDARIES defined)

---

**Modularity validation passed. Session may proceed to git operations.**
"""

print(modularity_report)
```

**Status**: ‚úÖ Phase 3 Complete

---

## Phase 4: Git Operations (Comprehensive)

**Purpose**: Stage all changes, create comprehensive commit, and tag the sprint.

**Status**: üìù Performing git operations...

### Step 3.1: Discover All Changed Files

```bash
echo "Discovering all changed files..."

# Get list of all changed files (new, modified, deleted)
git status --porcelain > /tmp/git_changes.txt

# Count changes
NEW_FILES=$(git status --porcelain | grep "^??" | wc -l | tr -d ' ')
MODIFIED_FILES=$(git status --porcelain | grep "^ M\|^M " | wc -l | tr -d ' ')
DELETED_FILES=$(git status --porcelain | grep "^ D\|^D " | wc -l | tr -d ' ')
TOTAL_CHANGES=$((NEW_FILES + MODIFIED_FILES + DELETED_FILES))

echo ""
echo "=============================================================================="
echo "üìÅ Changed Files Summary"
echo "=============================================================================="
echo "New files:      $NEW_FILES"
echo "Modified files: $MODIFIED_FILES"
echo "Deleted files:  $DELETED_FILES"
echo "Total changes:  $TOTAL_CHANGES"
echo "=============================================================================="
echo ""

if [ $TOTAL_CHANGES -eq 0 ]; then
    echo "‚ö†Ô∏è No changes detected. Nothing to commit."
    echo "Sprint closeout will create documentation only."
fi
```

### Step 3.2: Display Changed Files for Review

```bash
echo "Changed Files Detail:"
echo ""

# Show new files
if [ $NEW_FILES -gt 0 ]; then
    echo "NEW FILES:"
    git status --porcelain | grep "^??" | sed 's/^?? /  + /'
    echo ""
fi

# Show modified files
if [ $MODIFIED_FILES -gt 0 ]; then
    echo "MODIFIED FILES:"
    git status --porcelain | grep "^ M\|^M " | sed 's/^...../  ~ /'
    echo ""
fi

# Show deleted files
if [ $DELETED_FILES -gt 0 ]; then
    echo "DELETED FILES:"
    git status --porcelain | grep "^ D\|^D " | sed 's/^...../  - /'
    echo ""
fi
```

### Step 3.3: Get Git Statistics

```bash
# Get detailed git diff statistics
echo "Calculating code changes..."

# Get line counts (insertions and deletions)
DIFF_STATS=$(git diff --stat HEAD)
INSERTIONS=$(git diff --numstat HEAD | awk '{sum+=$1} END {print sum}')
DELETIONS=$(git diff --numstat HEAD | awk '{sum+=$2} END {print sum}')

# Handle empty values
INSERTIONS=${INSERTIONS:-0}
DELETIONS=${DELETIONS:-0}
NET_CHANGE=$((INSERTIONS - DELETIONS))

echo ""
echo "Code Changes:"
echo "  Lines added:   +$INSERTIONS"
echo "  Lines removed: -$DELETIONS"
echo "  Net change:    $NET_CHANGE"
echo ""
```

### Step 3.4: Generate Comprehensive Commit Message

```python
# Generate professional commit message
from datetime import datetime

# Get sprint info
sprint_name = "Sprint Name"  # Extract from sprint plan
sprint_goal = "Sprint Goal"  # Extract from sprint plan

# Extract key accomplishments from sprint summary
accomplishments = []
for story in stories:
    if story['status'] == 'complete':
        accomplishments.append(f"- {story['id']}: {story['title']}")

accomplishments_text = "\n".join(accomplishments) if accomplishments else "- No stories completed"

# Build commit message using Conventional Commits format
commit_message = f"""sprint(sprint-{sprint_id}): close sprint {sprint_id} - {sprint_name}

Summary:
- Completed {completed_points} of {total_points} planned story points ({completion_rate:.0f}%)
- Delivered {completed_stories} of {total_stories} user stories
- Code coverage: {total_coverage:.1f}%
- Test pass rate: 100%

Sprint Goal Achievement: {'‚úÖ Achieved' if completion_rate >= 85 else '‚ö†Ô∏è Partially Achieved' if completion_rate >= 50 else '‚ùå Not Achieved'}
Sprint Goal: {sprint_goal}

Key Accomplishments:
{accomplishments_text}

Metrics:
- Velocity: {completed_points} points
- Code Coverage: {total_coverage:.1f}%
- Test Pass Rate: 100%
- Files Changed: {TOTAL_CHANGES}
- Lines Added: +{INSERTIONS}
- Lines Removed: -{DELETIONS}
- Net Change: {'+' if NET_CHANGE >= 0 else ''}{NET_CHANGE}

Files Changed: {TOTAL_CHANGES} files, {INSERTIONS} insertions(+), {DELETIONS} deletions(-)

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
"""

# Save commit message to temp file
with open('/tmp/commit_message.txt', 'w') as f:
    f.write(commit_message)

print("‚úÖ Commit message generated")
```

### Step 3.5: Stage All Changes

```bash
echo "Staging all changes..."

# Add all files (new, modified, deleted)
git add .

# Verify staging
STAGED_FILES=$(git diff --cached --name-only | wc -l | tr -d ' ')

echo "‚úÖ Staged $STAGED_FILES files"
```

### Step 3.6: Create Commit

```bash
echo "Creating commit..."

# Create commit with comprehensive message
git commit -F /tmp/commit_message.txt

COMMIT_HASH=$(git rev-parse HEAD)
COMMIT_SHORT_HASH=$(git rev-parse --short HEAD)

echo ""
echo "=============================================================================="
echo "‚úÖ Commit Created"
echo "=============================================================================="
echo "Commit: $COMMIT_SHORT_HASH"
echo "Full Hash: $COMMIT_HASH"
echo ""
echo "Commit Message Preview:"
echo "------------------------------------------------------------------------------"
head -20 /tmp/commit_message.txt
echo "------------------------------------------------------------------------------"
echo ""
```

### Step 3.7: Create Sprint Tag

```bash
echo "Creating sprint tag..."

# Create annotated tag for the sprint
TAG_NAME="sprint-${sprint_id}"

git tag -a "$TAG_NAME" -m "Sprint $sprint_id closeout

Completed: $completed_stories/$total_stories stories ($completion_rate% completion)
Velocity: $completed_points points
Coverage: $total_coverage%

Tag created: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"

echo ""
echo "=============================================================================="
echo "‚úÖ Git Tag Created"
echo "=============================================================================="
echo "Tag: $TAG_NAME"
echo ""
echo "To push commit and tag to remote:"
echo "  git push origin main"
echo "  git push origin $TAG_NAME"
echo ""
echo "Or push both at once:"
echo "  git push origin main --tags"
echo "=============================================================================="
echo ""
```

**Status**: ‚úÖ Phase 4 Complete

---

## Phase 5: Session Documentation Updates

**Purpose**: Update session state and create next session plan.

**Status**: üìÑ Updating session documentation...

### Step 4.1: Update Current Session State

```python
# Load current session state
if os.path.exists(session_state_path):
    with open(session_state_path, 'r') as f:
        current_session = f.read()
else:
    current_session = ""

# Update with final accomplishments
final_session_content = f"""# Session Closeout - Sprint {sprint_id}

**Session Closed**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sprint**: S{sprint_id}
**Status**: ‚úÖ Complete

---

## Final Accomplishments

### Stories Completed
{accomplishments_text}

### Metrics
- **Velocity**: {completed_points} points
- **Completion Rate**: {completion_rate:.1f}%
- **Code Coverage**: {total_coverage:.1f}%
- **Test Pass Rate**: 100%

### Files Changed
- **Total Files**: {TOTAL_CHANGES}
- **Lines Added**: {INSERTIONS}
- **Lines Removed**: {DELETIONS}
- **Net Change**: {NET_CHANGE}

---

## Quality Validation

‚úÖ All tests passing (100% pass rate)
‚úÖ Code coverage meets threshold ({total_coverage:.1f}% >= 80%)
‚úÖ All completed stories meet Definition of Done
‚úÖ Documentation updated

---

## Git Operations

**Commit**: {COMMIT_SHORT_HASH}
**Tag**: sprint-{sprint_id}
**Commit Message**: Sprint {sprint_id} closeout

---

## Session Summary

This session successfully completed Sprint {sprint_id} with {completed_stories} of {total_stories} stories delivered.
{f'{incomplete_stories} stories remain incomplete and have been documented for the next sprint.' if incomplete_stories > 0 else 'All planned stories completed successfully.'}

Sprint closeout performed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.

---

## Next Session

See `state/next_session_plan.md` for next session priorities.
"""

with open(session_state_path, 'w') as f:
    f.write(final_session_content)

print(f"‚úÖ Updated: {session_state_path}")
```

### Step 4.2: Create Next Session Plan

```python
# Determine next sprint
next_sprint_id = str(int(sprint_id) + 1).zfill(2)
next_sprint_plan_path = f'sprints/sprint_{next_sprint_id}/sprint_plan.md'

# Check if next sprint plan exists
if os.path.exists(next_sprint_plan_path):
    with open(next_sprint_plan_path, 'r') as f:
        next_sprint_content = f.read()

    # Extract next sprint goal and top priorities
    next_sprint_goal_match = re.search(r'Sprint Goal:(.+)', next_sprint_content)
    next_sprint_goal = next_sprint_goal_match.group(1).strip() if next_sprint_goal_match else "Not defined"

    # Extract first 3 user stories
    next_stories = []
    for line in next_sprint_content.split('\n'):
        story_match = re.match(r'^###\s+(US-\d+):\s+(.+)', line)
        if story_match and len(next_stories) < 3:
            next_stories.append(f"- {story_match.group(1)}: {story_match.group(2)}")

    next_sprint_info = f"""

## Next Sprint: Sprint {next_sprint_id}

**Sprint Goal**: {next_sprint_goal}

**Top Priorities**:
{chr(10).join(next_stories) if next_stories else '- To be defined in sprint planning'}
"""
else:
    next_sprint_info = """

## Next Sprint

Next sprint plan not yet created. Run `/init` to create next sprint plan or wait for sprint planning session.
"""

# Create next session plan
next_session_plan_content = f"""# Next Session Plan

**Created**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Previous Sprint**: S{sprint_id} (Complete)
**Next Sprint**: S{next_sprint_id}

---

## Context from Previous Sprint

### Completed
- ‚úÖ {completed_stories} of {total_stories} stories completed
- ‚úÖ {completed_points} of {total_points} points delivered
- ‚úÖ {completion_rate:.1f}% completion rate

### Incomplete Work
"""

if incomplete_story_list:
    for story in incomplete_story_list:
        next_session_plan_content += f"- ‚è≥ {story['id']}: {story['title']} ({story['points']} pts)\n"
else:
    next_session_plan_content += "- None - all stories completed!\n"

next_session_plan_content += f"""
{next_sprint_info}

---

## Quick Start for Next Session

1. Run `/start` to begin next sprint
2. Review sprint plan: `sprints/sprint_{next_sprint_id}/sprint_plan.md`
3. Check for any blockers or dependencies
4. Begin with highest priority story

---

## Prerequisites for Next Session

- [ ] Next sprint plan exists and is reviewed
- [ ] Any blockers from previous sprint resolved
- [ ] Team capacity confirmed
- [ ] Dependencies identified and ready

---

## Technical Context

### Recent Decisions
[Will be populated during sprint execution]

### Known Issues
[Will be updated as issues are discovered]

### Environment Notes
[Will be updated as needed]

---

**Session continuity maintained**. Ready to start Sprint {next_sprint_id}.
"""

next_session_plan_path = 'state/next_session_plan.md'
with open(next_session_plan_path, 'w') as f:
    f.write(next_session_plan_content)

print(f"‚úÖ Created: {next_session_plan_path}")
```

**Status**: ‚úÖ Phase 5 Complete

---

## Phase 5.5: Module Registry Update

**Purpose**: Update the module registry with any modules added, modified, or removed during this sprint. This keeps the registry in sync and reduces grep operations in future sessions.

**Status**: üîÑ Updating module registry...

### Step 5.5.1: Check for Module Registry

```python
import os
import re
import ast
from datetime import datetime
from pathlib import Path

print("\n" + "="*80)
print("üì¶ MODULE REGISTRY UPDATE")
print("="*80)

registry_path = 'state/modules_registry.md'

if not os.path.exists(registry_path):
    print("‚ÑπÔ∏è  No module registry found - skipping update")
    print("   Run /session:init to create the module registry")
    registry_updated = False
else:
    with open(registry_path, 'r') as f:
        registry_content = f.read()

    # Extract project language
    lang_match = re.search(r'Language:\s*(\w+)', registry_content)
    project_language = lang_match.group(1).lower() if lang_match else "python"
    print(f"\nüìã Registry found (Language: {project_language})")
```

### Step 5.5.2: Identify Changed Modules

```python
    import subprocess

    # Get files changed in this sprint (from last commit or uncommitted)
    diff_result = subprocess.run(
        ["git", "diff", "HEAD~1", "--name-only", "--diff-filter=ACMRD"],
        capture_output=True,
        text=True
    )

    if diff_result.returncode != 0:
        # Fallback to status
        diff_result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True
        )
        changed_files = [
            line[3:].strip() for line in diff_result.stdout.strip().split('\n')
            if line.strip()
        ]
    else:
        changed_files = [f for f in diff_result.stdout.strip().split('\n') if f]

    # Filter to module files
    if project_language == "python":
        module_patterns = ['__init__.py', '.py']
        exclude_patterns = ['test_', '_test.py', 'tests/', 'conftest.py']
    else:  # TypeScript
        module_patterns = ['index.ts', 'index.tsx', '.ts', '.tsx']
        exclude_patterns = ['.test.', '.spec.', '__tests__/', 'test/']

    changed_modules = []
    for f in changed_files:
        if any(p in f for p in exclude_patterns):
            continue
        if any(f.endswith(p) for p in module_patterns):
            # Extract module path
            module_path = str(Path(f).parent)
            if module_path not in changed_modules and module_path != '.':
                changed_modules.append(module_path)

    print(f"\nüìÇ Modules to update: {len(changed_modules)}")
    for m in changed_modules[:10]:
        print(f"   - {m}")
    if len(changed_modules) > 10:
        print(f"   ... and {len(changed_modules) - 10} more")
```

### Step 5.5.3: Extract Module Interfaces

```python
    def extract_python_interface(module_path):
        """Extract public functions and classes from Python module."""
        interfaces = []
        init_path = os.path.join(module_path, '__init__.py')

        # Find all Python files in module
        py_files = list(Path(module_path).glob('*.py'))

        for py_file in py_files:
            if py_file.name.startswith('_') and py_file.name != '__init__.py':
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    source = f.read()

                tree = ast.parse(source)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef) and not node.name.startswith('_'):
                        # Build function signature
                        args = []
                        for arg in node.args.args:
                            arg_str = arg.arg
                            if arg.annotation:
                                try:
                                    arg_str += f": {ast.unparse(arg.annotation)}"
                                except:
                                    pass
                            args.append(arg_str)

                        returns = ""
                        if node.returns:
                            try:
                                returns = f" -> {ast.unparse(node.returns)}"
                            except:
                                pass

                        # Get docstring
                        docstring = ast.get_docstring(node) or ""
                        if docstring:
                            docstring = docstring.split('\n')[0][:60]

                        interfaces.append({
                            'type': 'function',
                            'name': node.name,
                            'signature': f"def {node.name}({', '.join(args)}){returns}",
                            'docstring': docstring,
                            'file': py_file.name
                        })

                    elif isinstance(node, ast.ClassDef) and not node.name.startswith('_'):
                        docstring = ast.get_docstring(node) or ""
                        if docstring:
                            docstring = docstring.split('\n')[0][:60]

                        interfaces.append({
                            'type': 'class',
                            'name': node.name,
                            'signature': f"class {node.name}",
                            'docstring': docstring,
                            'file': py_file.name
                        })

            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not parse {py_file}: {e}")

        return interfaces

    def extract_typescript_interface(module_path):
        """Extract exported functions and classes from TypeScript module."""
        interfaces = []

        # Find TypeScript files
        ts_files = list(Path(module_path).glob('*.ts')) + list(Path(module_path).glob('*.tsx'))

        for ts_file in ts_files:
            if ts_file.name.startswith('_'):
                continue

            try:
                with open(ts_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Extract exported functions
                func_pattern = r'export\s+(?:async\s+)?function\s+(\w+)\s*(<[^>]+>)?\s*\(([^)]*)\)\s*(?::\s*([^{]+))?'
                for match in re.finditer(func_pattern, content):
                    name = match.group(1)
                    generics = match.group(2) or ""
                    params = match.group(3).strip()
                    returns = match.group(4).strip() if match.group(4) else "void"

                    interfaces.append({
                        'type': 'function',
                        'name': name,
                        'signature': f"function {name}{generics}({params}): {returns}",
                        'docstring': "",
                        'file': ts_file.name
                    })

                # Extract exported classes
                class_pattern = r'export\s+class\s+(\w+)(?:\s+extends\s+(\w+))?(?:\s+implements\s+([^{]+))?'
                for match in re.finditer(class_pattern, content):
                    name = match.group(1)
                    extends = f" extends {match.group(2)}" if match.group(2) else ""
                    implements = f" implements {match.group(3).strip()}" if match.group(3) else ""

                    interfaces.append({
                        'type': 'class',
                        'name': name,
                        'signature': f"class {name}{extends}{implements}",
                        'docstring': "",
                        'file': ts_file.name
                    })

                # Extract exported interfaces/types
                type_pattern = r'export\s+(?:interface|type)\s+(\w+)(?:<[^>]+>)?'
                for match in re.finditer(type_pattern, content):
                    name = match.group(1)
                    interfaces.append({
                        'type': 'type',
                        'name': name,
                        'signature': f"type {name}",
                        'docstring': "",
                        'file': ts_file.name
                    })

            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not parse {ts_file}: {e}")

        return interfaces
```

### Step 5.5.4: Update Registry File

```python
    # Parse existing registry to find module sections
    module_sections = {}
    current_module = None
    section_start = None

    lines = registry_content.split('\n')
    for i, line in enumerate(lines):
        if line.startswith('### ') and not line.startswith('### Quick') and not line.startswith('### Public'):
            if current_module and section_start is not None:
                module_sections[current_module] = (section_start, i - 1)
            current_module = line[4:].strip()
            section_start = i

    if current_module and section_start is not None:
        module_sections[current_module] = (section_start, len(lines) - 1)

    # Track updates
    modules_updated = []
    modules_added = []

    for module_path in changed_modules:
        if not os.path.isdir(module_path):
            continue

        module_name = Path(module_path).name

        # Extract interfaces
        if project_language == "python":
            interfaces = extract_python_interface(module_path)
        else:
            interfaces = extract_typescript_interface(module_path)

        if not interfaces:
            continue

        # Build module section
        section_lines = [
            f"### {module_name}",
            f"**Path**: `{module_path}/`",
            f"**Last Updated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "",
            "#### Public Interface"
        ]

        # Add code block for interfaces
        if project_language == "python":
            section_lines.append("```python")
        else:
            section_lines.append("```typescript")

        for iface in interfaces:
            if iface['docstring']:
                section_lines.append(f"{iface['signature']}:")
                section_lines.append(f'    """{iface["docstring"]}"""')
            else:
                section_lines.append(iface['signature'])
            section_lines.append("")

        section_lines.append("```")
        section_lines.append("")

        new_section = '\n'.join(section_lines)

        # Check if module exists in registry
        if module_name in module_sections:
            # Update existing section
            start, end = module_sections[module_name]
            lines[start:end+1] = section_lines
            modules_updated.append(module_name)
        else:
            # Add new module section before "## Registry Maintenance Log"
            log_match = re.search(r'## Registry Maintenance Log', registry_content)
            if log_match:
                insert_pos = registry_content.rfind('\n', 0, log_match.start())
                registry_content = (
                    registry_content[:insert_pos] +
                    '\n\n' + new_section +
                    registry_content[insert_pos:]
                )
            modules_added.append(module_name)

    # Reconstruct registry if we modified line by line
    if modules_updated:
        registry_content = '\n'.join(lines)

    # Update Quick Stats
    module_count = len(re.findall(r'^### (?!Quick|Public)', registry_content, re.MULTILINE))
    interface_count = len(re.findall(r'^(def |function |class |type )', registry_content, re.MULTILINE))

    registry_content = re.sub(
        r'Total Modules:\s*\d+',
        f'Total Modules: {module_count}',
        registry_content
    )
    registry_content = re.sub(
        r'Public Interfaces:\s*\d+',
        f'Public Interfaces: {interface_count}',
        registry_content
    )

    # Add maintenance log entry
    log_entry = f"| {datetime.now().strftime('%Y-%m-%d %H:%M')} | Updated | {', '.join(modules_updated + modules_added)[:50]} | /session:end |"

    log_match = re.search(r'(\| Date \| Action \| Module \| By \|\n\|[-|]+\|)', registry_content)
    if log_match:
        insert_pos = log_match.end()
        registry_content = (
            registry_content[:insert_pos] +
            '\n' + log_entry +
            registry_content[insert_pos:]
        )

    # Write updated registry
    with open(registry_path, 'w') as f:
        f.write(registry_content)

    registry_updated = True

    print(f"\n‚úÖ Module registry updated:")
    print(f"   - Modules updated: {len(modules_updated)}")
    print(f"   - Modules added: {len(modules_added)}")
    print(f"   - Total modules: {module_count}")
    print(f"   - Total interfaces: {interface_count}")

    if modules_updated:
        print(f"\n   Updated: {', '.join(modules_updated)}")
    if modules_added:
        print(f"   Added: {', '.join(modules_added)}")
```

**Status**: ‚úÖ Phase 5.5 Complete

---

## Phase 6: Sprint Summary Creation

**Purpose**: Create comprehensive sprint summary with accomplishments, metrics, and insights.

**Status**: üìä Creating sprint summary...

### Step 5.1: Extract Sprint Metadata

```python
# Extract sprint name and goal from sprint plan
sprint_name_match = re.search(r'# Sprint \d+:\s*(.+)', sprint_plan_content)
sprint_name = sprint_name_match.group(1).strip() if sprint_name_match else f"Sprint {sprint_id}"

sprint_goal_match = re.search(r'Sprint Goal:(.+?)(?:\n|$)', sprint_plan_content, re.MULTILINE)
sprint_goal = sprint_goal_match.group(1).strip() if sprint_goal_match else "Not specified"

# Extract sprint dates if available
dates_match = re.search(r'Dates?:\s*(.+?)\s*(?:to|-)\s*(.+?)(?:\n|$)', sprint_plan_content)
if dates_match:
    sprint_start = dates_match.group(1).strip()
    sprint_end = dates_match.group(2).strip()
else:
    sprint_start = "Not specified"
    sprint_end = datetime.now().strftime('%Y-%m-%d')
```

### Step 5.2: Get Git File Change Statistics

```python
import subprocess

# Get list of files changed in this sprint
try:
    # Get all commits since last sprint tag (or all if first sprint)
    last_tag = f"sprint-{str(int(sprint_id) - 1).zfill(2)}"

    # Try to get commits since last sprint
    result = subprocess.run(
        ['git', 'log', f'{last_tag}..HEAD', '--pretty=format:%H'],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        # First sprint or no previous tag, get all commits
        result = subprocess.run(
            ['git', 'log', '--pretty=format:%H'],
            capture_output=True,
            text=True
        )

    commit_hashes = result.stdout.strip().split('\n') if result.stdout else []

    # Get file statistics
    stats_result = subprocess.run(
        ['git', 'diff', '--stat', f'{last_tag}..HEAD'] if last_tag else ['git', 'diff', '--stat', 'HEAD'],
        capture_output=True,
        text=True
    )

    file_changes_summary = stats_result.stdout if stats_result.returncode == 0 else "Not available"

    # Get changed files list
    files_result = subprocess.run(
        ['git', 'diff', '--name-status', f'{last_tag}..HEAD'] if last_tag else ['git', 'diff', '--name-status', 'HEAD'],
        capture_output=True,
        text=True
    )

    changed_files = []
    if files_result.returncode == 0:
        for line in files_result.stdout.strip().split('\n'):
            if line:
                parts = line.split('\t')
                if len(parts) >= 2:
                    status = parts[0]
                    filepath = parts[1]
                    changed_files.append({
                        'status': status,
                        'path': filepath
                    })

except Exception as e:
    commit_hashes = []
    file_changes_summary = "Error retrieving git statistics"
    changed_files = []
```

### Step 5.3: Categorize Accomplishments

```python
# Categorize stories by type
features = [s for s in stories if s['status'] == 'complete' and not any(word in s['title'].lower() for word in ['fix', 'bug', 'refactor', 'test'])]
bugs = [s for s in stories if s['status'] == 'complete' and any(word in s['title'].lower() for word in ['fix', 'bug'])]
improvements = [s for s in stories if s['status'] == 'complete' and any(word in s['title'].lower() for word in ['refactor', 'improve', 'optimize', 'enhance'])]

# Build accomplishments sections
features_text = ""
if features:
    features_text = "\n### Features Delivered\n\n"
    for story in features:
        features_text += f"**{story['id']}: {story['title']}** ({story['points']} pts)\n"
        if story['acceptance_criteria']:
            features_text += "- Acceptance Criteria:\n"
            for criterion in story['acceptance_criteria']:
                if criterion['completed']:
                    features_text += f"  - ‚úÖ {criterion['text']}\n"
        features_text += "\n"

bugs_text = ""
if bugs:
    bugs_text = "\n### Bug Fixes\n\n"
    for story in bugs:
        bugs_text += f"**{story['id']}: {story['title']}** ({story['points']} pts)\n\n"

improvements_text = ""
if improvements:
    improvements_text = "\n### Technical Improvements\n\n"
    for story in improvements:
        improvements_text += f"**{story['id']}: {story['title']}** ({story['points']} pts)\n\n"
```

### Step 5.4: Generate Comprehensive Sprint Summary

```python
# Calculate sprint duration
try:
    start_date = datetime.strptime(sprint_start, '%Y-%m-%d')
    end_date = datetime.strptime(sprint_end, '%Y-%m-%d')
    duration_days = (end_date - start_date).days
    duration_weeks = duration_days / 7
except:
    duration_days = 14
    duration_weeks = 2

# Build comprehensive sprint summary
sprint_summary = f"""# Sprint {sprint_id} Summary - {sprint_name}

**Sprint Dates**: {sprint_start} - {sprint_end}
**Sprint Duration**: {duration_weeks:.1f} weeks ({duration_days} days)
**Team**: Development Team
**Closeout Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## Executive Summary

Sprint {sprint_id} {'successfully achieved' if completion_rate >= 85 else 'partially achieved' if completion_rate >= 50 else 'did not achieve'} its goal with {completed_stories} of {total_stories} planned stories completed ({completion_rate:.0f}% completion rate).

**Sprint Goal**: {sprint_goal}
**Goal Status**: {'‚úÖ Achieved' if completion_rate >= 85 else '‚ö†Ô∏è Partially Achieved' if completion_rate >= 50 else '‚ùå Not Achieved'}

---

## Sprint Metrics

### Velocity & Completion
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Story Points Committed | {total_points} | - | - |
| Story Points Completed | {completed_points} | {total_points} | {'‚úÖ' if completed_points >= total_points * 0.85 else '‚ö†Ô∏è'} |
| Sprint Completion Rate | {completion_rate:.1f}% | 85% | {'‚úÖ' if completion_rate >= 85 else '‚ö†Ô∏è'} |
| Stories Completed | {completed_stories}/{total_stories} | - | - |
| Velocity | {completed_points} pts | - | - |

### Quality Metrics
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Code Coverage | {total_coverage:.1f}% | 80% | ‚úÖ |
| Test Pass Rate | 100% | 100% | ‚úÖ |
| Build Success | ‚úÖ Pass | Pass | ‚úÖ |
| Test Quality Score | {test_quality_metrics.get('score', 'N/A')}/100 | 70+ | {'‚úÖ' if test_quality_metrics.get('score', 0) >= 70 else '‚ö†Ô∏è'} |
| Code Quality Grade | {code_quality_metrics.get('grade', 'N/A')} | C+ | {'‚úÖ' if code_quality_metrics.get('grade', 'F') in ['A', 'B', 'C'] else '‚ö†Ô∏è'} |

### Code Changes
| Metric | Value |
|--------|-------|
| Files Changed | {len(changed_files)} |
| Lines Added | +{INSERTIONS} |
| Lines Removed | -{DELETIONS} |
| Net Change | {'+' if NET_CHANGE >= 0 else ''}{NET_CHANGE} |
| Commits | {len(commit_hashes)} |

---

## Accomplishments

{features_text}
{bugs_text}
{improvements_text}

---

## Files Changed

### Summary
```
{file_changes_summary}
```

### Key Files Modified
"""

# Add top changed files
for i, file_info in enumerate(changed_files[:10]):
    status_map = {'A': 'Added', 'M': 'Modified', 'D': 'Deleted', 'R': 'Renamed'}
    status_text = status_map.get(file_info['status'][0], file_info['status'])
    sprint_summary += f"- `{file_info['path']}` - {status_text}\n"

if len(changed_files) > 10:
    sprint_summary += f"\n_... and {len(changed_files) - 10} more files_\n"

sprint_summary += f"""

---

## Testing Summary

### Test Coverage
- **Total Coverage**: {total_coverage:.2f}%
- **Test Pass Rate**: 100%
- **Tests Passing**: All

### Test Quality Assessment
- **Test Quality Score**: {test_quality_metrics.get('score', 'N/A')}/100 (Grade: {test_quality_metrics.get('grade', 'N/A')})
- **Total Test Files**: {test_quality_metrics.get('total_files', 'N/A')}
- **Total Tests**: {test_quality_metrics.get('total_tests', 'N/A')}
- **Assertion Density**: {test_quality_metrics.get('assertion_density', 'N/A'):.1f} assertions/test

**Anti-Pattern Detection**:
| Pattern | Count | Severity |
|---------|-------|----------|
| Sleep in Tests | {test_quality_metrics.get('anti_patterns', {}).get('sleep_in_tests', 0)} | HIGH |
| Assertion-Free Tests | {test_quality_metrics.get('anti_patterns', {}).get('assertion_free_tests', 0)} | HIGH |
| Trivial Assertions | {test_quality_metrics.get('anti_patterns', {}).get('trivial_assertions', 0)} | MEDIUM |
| Mock Overuse | {test_quality_metrics.get('anti_patterns', {}).get('mock_overuse', 0)} | MEDIUM |
| Poor Test Names | {test_quality_metrics.get('anti_patterns', {}).get('poor_test_names', 0)} | LOW |

### Code Quality Self-Reflection
- **Overall Score**: {code_quality_metrics.get('overall_score', 'N/A')}/10 (Grade: {code_quality_metrics.get('grade', 'N/A')})
- **Files Evaluated**: {code_quality_metrics.get('files_evaluated', 'N/A')}
- **Lines Evaluated**: {code_quality_metrics.get('lines_evaluated', 'N/A')}
- **Second Pass Recommended**: {'Yes' if code_quality_metrics.get('second_pass_recommended', False) else 'No'}

**Dimension Breakdown**:
| Dimension | Score |
|-----------|-------|
| Readability | {code_quality_metrics.get('dimensions', {}).get('readability', 'N/A')}/10 |
| Maintainability | {code_quality_metrics.get('dimensions', {}).get('maintainability', 'N/A')}/10 |
| Test Quality | {code_quality_metrics.get('dimensions', {}).get('test_quality', 'N/A')}/10 |
| Error Handling | {code_quality_metrics.get('dimensions', {}).get('error_handling', 'N/A')}/10 |
| Documentation | {code_quality_metrics.get('dimensions', {}).get('documentation', 'N/A')}/10 |
| Architecture | {code_quality_metrics.get('dimensions', {}).get('architecture', 'N/A')}/10 |

### Quality Summary
- ‚úÖ All unit tests passing
- ‚úÖ All integration tests passing
- ‚úÖ Code coverage meets threshold
- {'‚úÖ' if test_quality_metrics.get('score', 0) >= 70 else '‚ö†Ô∏è'} Test quality {'meets' if test_quality_metrics.get('score', 0) >= 70 else 'below'} target
- {'‚úÖ' if code_quality_metrics.get('grade', 'F') in ['A', 'B', 'C'] else '‚ö†Ô∏è'} Code quality {'meets' if code_quality_metrics.get('grade', 'F') in ['A', 'B', 'C'] else 'below'} threshold

---

## Incomplete Work

"""

if incomplete_story_list:
    sprint_summary += f"**{len(incomplete_story_list)} stories moved to backlog:**\n\n"
    for story in incomplete_story_list:
        sprint_summary += f"### {story['id']}: {story['title']} ({story['points']} pts)\n"
        sprint_summary += f"**Status**: {story['status']}\n"
        sprint_summary += f"**Reason**: See `sprints/sprint_{sprint_id}/incomplete_work.md` for details\n\n"
else:
    sprint_summary += "‚úÖ All planned stories completed - no incomplete work!\n\n"

sprint_summary += f"""
---

## Technical Decisions

[Document any significant technical decisions made during the sprint]

_Technical decisions and architectural choices will be documented here during sprint execution._

---

## Blockers & Resolutions

### Blockers Resolved
[Document blockers that were encountered and resolved]

### Outstanding Blockers
"""

if incomplete_story_list:
    sprint_summary += "[Document any remaining blockers that prevented story completion]\n\n"
else:
    sprint_summary += "None - all blockers resolved\n\n"

sprint_summary += f"""
---

## Sprint Retrospective Preview

**What Went Well** ‚úÖ
- Achieved {completion_rate:.0f}% story completion
- Maintained {total_coverage:.1f}% code coverage
- All tests passing
[To be completed in retrospective session]

**What Could Be Improved** ‚ö†Ô∏è
[To be completed in retrospective session]

**Action Items** üéØ
[To be defined in retrospective session]

See `sprints/sprint_{sprint_id}/retrospective_template.md` for full retrospective.

---

## Next Sprint Preview

"""

if os.path.exists(next_sprint_plan_path):
    sprint_summary += f"""**Sprint {next_sprint_id}**: {next_sprint_goal}

**Top Priorities**:
{chr(10).join(next_stories) if next_stories else '- To be defined'}

"""
else:
    sprint_summary += "Next sprint plan to be created in upcoming sprint planning session.\n\n"

sprint_summary += f"""
---

## Artifacts & Links

- **Sprint Plan**: `sprints/sprint_{sprint_id}/sprint_plan.md`
- **Verification Document**: `sprints/sprint_{sprint_id}/verification.md`
- **Retrospective Template**: `sprints/sprint_{sprint_id}/retrospective_template.md`
- **Metrics Dashboard**: `sprints/sprint_{sprint_id}/metrics.md`
- **Git Tag**: `sprint-{sprint_id}`
- **Commit**: `{COMMIT_SHORT_HASH}`

---

**Prepared by**: Claude Code
**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sprint Status**: ‚úÖ Complete
"""

# Write sprint summary
summary_path = f'sprints/sprint_{sprint_id}/sprint_summary.md'
os.makedirs(os.path.dirname(summary_path), exist_ok=True)

with open(summary_path, 'w') as f:
    f.write(sprint_summary)

print(f"‚úÖ Sprint summary created: {summary_path}")
```

**Status**: ‚úÖ Phase 6 Complete

---

## Phase 7: CLAUDE.md Roadmap Update

**Purpose**: Update roadmap section in CLAUDE.md with sprint accomplishments.

**Status**: üó∫Ô∏è Updating roadmap...

### Step 6.1: Load and Parse CLAUDE.md

```python
# Load CLAUDE.md
with open('CLAUDE.md', 'r') as f:
    claude_content = f.read()

# Find roadmap section
roadmap_start = claude_content.find('# Roadmap')
if roadmap_start == -1:
    roadmap_start = claude_content.find('## Roadmap')

if roadmap_start == -1:
    print("‚ö†Ô∏è Warning: No roadmap section found in CLAUDE.md")
    print("Creating roadmap section...")
    roadmap_section = "\n\n# Roadmap\n\n"
else:
    # Extract roadmap section
    next_section = claude_content.find('\n# ', roadmap_start + 1)
    if next_section == -1:
        next_section = claude_content.find('\n## ', roadmap_start + 1)

    if next_section != -1:
        roadmap_section = claude_content[roadmap_start:next_section]
    else:
        roadmap_section = claude_content[roadmap_start:]
```

### Step 6.2: Update Sprint Progress

```python
# Update sprint completion in roadmap
updated_roadmap = roadmap_section

# Update current sprint status
sprint_marker = f"Sprint {sprint_id}"
if sprint_marker in updated_roadmap:
    # Update existing sprint entry
    updated_roadmap = re.sub(
        f'(Sprint {sprint_id}.*?)‚è≥',
        f'\\1‚úÖ',
        updated_roadmap
    )
    updated_roadmap = re.sub(
        f'(Sprint {sprint_id}.*?)\\(\\d+%\\)',
        f'\\1(100%)',
        updated_roadmap
    )
else:
    # Add sprint completion entry
    sprint_entry = f"\n- **Sprint {sprint_id}**: {sprint_name} - ‚úÖ Complete ({completion_rate:.0f}% completion)\n"

    # Add after roadmap header
    header_end = updated_roadmap.find('\n', updated_roadmap.find('Roadmap'))
    if header_end != -1:
        updated_roadmap = updated_roadmap[:header_end] + sprint_entry + updated_roadmap[header_end:]
```

### Step 6.3: Update Feature Status

```python
# Update feature completion status
for story in features:
    # Try to find and update feature in roadmap
    feature_keywords = story['title'].lower().split()[:3]

    for keyword in feature_keywords:
        if len(keyword) > 4:  # Only use meaningful words
            # Find feature mention in roadmap
            pattern = f'({keyword}.*?)‚è≥'
            updated_roadmap = re.sub(pattern, '\\1‚úÖ', updated_roadmap, flags=re.IGNORECASE)

# Add completed features if not present
if features:
    # Check if there's a "Completed Features" section
    if "Completed Features" not in updated_roadmap and "Recent Accomplishments" not in updated_roadmap:
        completed_section = "\n## Recent Accomplishments\n\n"
        for story in features[:5]:  # Top 5 features
            completed_section += f"- ‚úÖ {story['title']} (Sprint {sprint_id})\n"

        # Add after first paragraph or header
        insert_pos = updated_roadmap.find('\n\n')
        if insert_pos != -1:
            updated_roadmap = updated_roadmap[:insert_pos] + completed_section + updated_roadmap[insert_pos:]
```

### Step 6.4: Update Current Sprint Marker

```python
# Update "Current Sprint" marker
updated_roadmap = re.sub(
    r'Current Sprint:?\s*S?\d+',
    f'Current Sprint: S{next_sprint_id}',
    updated_roadmap,
    flags=re.IGNORECASE
)

# Update overall progress
# Calculate overall project progress if possible
total_sprints_match = re.search(r'Total Sprints:?\s*(\d+)', updated_roadmap)
if total_sprints_match:
    total_sprints = int(total_sprints_match.group(1))
    overall_progress = (int(sprint_id) / total_sprints * 100)

    updated_roadmap = re.sub(
        r'Overall Progress:?\s*\d+%',
        f'Overall Progress: {overall_progress:.0f}%',
        updated_roadmap
    )
```

### Step 6.5: Write Updated CLAUDE.md

```python
# Replace roadmap section in CLAUDE.md
if roadmap_start != -1:
    if next_section != -1:
        new_claude_content = claude_content[:roadmap_start] + updated_roadmap + claude_content[next_section:]
    else:
        new_claude_content = claude_content[:roadmap_start] + updated_roadmap
else:
    # Append roadmap section
    new_claude_content = claude_content + updated_roadmap

# Write updated CLAUDE.md
with open('CLAUDE.md', 'w') as f:
    f.write(new_claude_content)

print("‚úÖ CLAUDE.md roadmap updated")
print(f"  - Sprint {sprint_id} marked as complete")
print(f"  - Current sprint updated to S{next_sprint_id}")
print(f"  - {len(features)} features marked as complete")
```

**Status**: ‚úÖ Phase 7 Complete

---

## Phase 8: Retrospective Template Preparation

**Purpose**: Create pre-populated retrospective template for team session.

**Status**: üîÑ Preparing retrospective template...

### Step 7.1: Generate Retrospective Template

```python
# Create comprehensive retrospective template
retrospective_content = f"""# Sprint {sprint_id} Retrospective

**Date**: {datetime.now().strftime('%Y-%m-%d')} (To be conducted)
**Sprint**: Sprint {sprint_id} - {sprint_name}
**Sprint Dates**: {sprint_start} - {sprint_end}
**Facilitator**: [Name]
**Participants**: [Team members]

---

## Sprint Overview

**Sprint Goal**: {sprint_goal}
**Goal Achievement**: {'‚úÖ Achieved' if completion_rate >= 85 else '‚ö†Ô∏è Partially Achieved' if completion_rate >= 50 else '‚ùå Not Achieved'}

### Key Metrics

| Metric | Value | Previous Sprint | Trend |
|--------|-------|----------------|-------|
| Velocity | {completed_points} pts | [Previous] | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |
| Completion Rate | {completion_rate:.0f}% | [Previous] | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |
| Code Coverage | {total_coverage:.1f}% | [Previous] | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |
| Stories Completed | {completed_stories}/{total_stories} | [Previous] | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |

---

## What Went Well ‚úÖ

_Team to discuss and document what worked well this sprint:_

1. **[Achievement/Practice]**
   - Why it worked:
   - Impact:
   - Should continue: Yes/No

2. **[Achievement/Practice]**
   - Why it worked:
   - Impact:
   - Should continue: Yes/No

3. **[Achievement/Practice]**
   - Why it worked:
   - Impact:
   - Should continue: Yes/No

**Pre-populated observations**:
- ‚úÖ Achieved {completion_rate:.0f}% story completion
- ‚úÖ Maintained {total_coverage:.1f}% code coverage (target: 80%)
- ‚úÖ All tests passing throughout sprint
- ‚úÖ {completed_stories} stories delivered

---

## What Could Be Improved ‚ö†Ô∏è

_Team to discuss and document areas for improvement:_

1. **[Issue/Challenge]**
   - Impact:
   - Root Cause:
   - Frequency: One-time/Recurring
   - Severity: High/Medium/Low

2. **[Issue/Challenge]**
   - Impact:
   - Root Cause:
   - Frequency: One-time/Recurring
   - Severity: High/Medium/Low

**Pre-populated observations**:
"""

if incomplete_story_list:
    retrospective_content += f"- ‚ö†Ô∏è {len(incomplete_story_list)} stories incomplete - discuss estimation or scope\n"

retrospective_content += f"""
---

## Insights & Learnings üí°

### Technical Learnings
1. **[Learning]**:
   - Application: How to apply in future sprints
   - Shared With: Team/Organization

### Process Learnings
1. **[Learning]**:
   - Application: How to apply in future sprints

### Team Dynamics
- [Observation]
- [Observation]

---

## Action Items üéØ

_Specific, actionable improvements for next sprint:_

### High Priority
1. **[Action]**: [Clear, specific description]
   - **Owner**: [Name]
   - **Due**: Sprint [N] / [Date]
   - **Success Criteria**: [How we'll know it's complete]
   - **Status**: Not Started

2. **[Action]**: [Clear, specific description]
   - **Owner**: [Name]
   - **Due**: Sprint [N] / [Date]
   - **Success Criteria**: [How we'll know it's complete]
   - **Status**: Not Started

### Medium Priority
1. **[Action]**: [Clear, specific description]
   - **Owner**: [Name]
   - **Due**: Sprint [N] / [Date]
   - **Success Criteria**: [How we'll know it's complete]
   - **Status**: Not Started

---

## Previous Action Items Review

_Review action items from Sprint {str(int(sprint_id) - 1).zfill(2)} retrospective:_

| Action | Owner | Status | Notes |
|--------|-------|--------|-------|
| [Previous action 1] | [Name] | [‚úÖ/üîÑ/‚ùå] | [Outcome] |
| [Previous action 2] | [Name] | [‚úÖ/üîÑ/‚ùå] | [Outcome] |

**Completion Rate**: [X]% ([N] of [M] completed)

---

## Team Health Indicators

### Collaboration Quality: [1-5] ‚≠ê
**Comments**:

### Work-Life Balance: [1-5] ‚≠ê
**Comments**:

### Psychological Safety: [1-5] ‚≠ê
**Comments**:

### Technical Environment: [1-5] ‚≠ê
**Comments**:

---

## Appreciations üôè

_Team members can recognize each other's contributions:_

- **[Person]**: [What they did and why it mattered]
- **[Person]**: [What they did and why it mattered]

---

## Additional Notes

[Free-form notes from discussion]

---

## Follow-up

**Next Retrospective**: Sprint {next_sprint_id} closeout
**Action Item Review**: [When/How often]
**Distribution**: [Who receives this document]

---

**Retrospective Status**: ‚è≥ To Be Conducted
**Template Created**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Metrics Pre-populated**: ‚úÖ Yes
"""

# Write retrospective template
retro_path = f'sprints/sprint_{sprint_id}/retrospective_template.md'
with open(retro_path, 'w') as f:
    f.write(retrospective_content)

print(f"‚úÖ Retrospective template created: {retro_path}")
print("  - Sprint metrics pre-populated")
print("  - Sections ready for team input")
print("  - Previous action items placeholder included")
```

**Status**: ‚úÖ Phase 8 Complete

---

## Phase 9: Metrics Dashboard Generation

**Purpose**: Generate visual metrics dashboard for sprint analysis.

**Status**: üìà Generating metrics dashboard...

### Step 8.1: Create Metrics Dashboard

```python
# Create comprehensive metrics dashboard
metrics_content = f"""# Sprint {sprint_id} Metrics Dashboard

**Sprint**: {sprint_name}
**Period**: {sprint_start} - {sprint_end}
**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## Executive Summary

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  SPRINT {sprint_id} PERFORMANCE DASHBOARD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  Velocity:          {completed_points} pts
  Completion:        {completion_rate:.0f}%
  Quality:           {total_coverage:.1f}% coverage, 100% pass rate
  Status:            {'‚úÖ SUCCESS' if completion_rate >= 85 else '‚ö†Ô∏è  PARTIAL' if completion_rate >= 50 else '‚ùå MISS'}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

---

## Velocity & Throughput

### Story Points
```
Planned:   {total_points} pts  {'‚ñà' * min(total_points, 20)}
Completed: {completed_points} pts  {'‚ñà' * min(completed_points, 20)}
Rate:      {completion_rate:.0f}%
```

### Stories
```
Total:     {total_stories} stories
Complete:  {completed_stories} stories  ({'‚úÖ' * completed_stories}{'‚¨ú' * (total_stories - completed_stories)})
Progress:  {completed_stories}/{total_stories} ({completion_rate:.0f}%)
```

---

## Quality Metrics

### Code Coverage
```
Current:  {total_coverage:.1f}%  {'‚ñà' * int(total_coverage / 5)}{'‚ñë' * (20 - int(total_coverage / 5))}
Target:   80.0%   {'‚ñà' * 16}{'‚ñë' * 4}
Status:   {'‚úÖ PASS' if total_coverage >= 80.0 else '‚ùå FAIL'}
```

### Test Results
```
Pass Rate:   100%    {'‚ñà' * 20}
Tests:       All Passing ‚úÖ
Build:       Success ‚úÖ
```

---

## Code Changes

### Files Modified
```
Total Files:  {len(changed_files)}
Added:        [Count from git]
Modified:     [Count from git]
Deleted:      [Count from git]
```

### Lines of Code
```
Insertions:  +{INSERTIONS}  {'‚ñà' * min(INSERTIONS // 100, 20)}
Deletions:   -{DELETIONS}  {'‚ñà' * min(DELETIONS // 100, 20)}
Net Change:  {'+' if NET_CHANGE >= 0 else ''}{NET_CHANGE}
```

---

## Sprint Burndown

_Sprint burndown chart (points remaining per day):_

```
Points
{total_points} ‚îÇ
       ‚îÇ ‚ï≤
       ‚îÇ  ‚ï≤
       ‚îÇ   ‚ï≤
       ‚îÇ    ‚ï≤
       ‚îÇ     ‚ï≤
       ‚îÇ      ‚ï≤
       ‚îÇ       ‚ï≤
     0 ‚îÇ________‚ï≤_______
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Days
       1  2  3  4  5  ...

[Actual data to be tracked during sprint]
```

---

## Story Breakdown

### By Status
- ‚úÖ Complete: {completed_stories} ({completed_stories/total_stories*100:.0f}%)
- üîÑ In Progress: {in_progress_stories} ({in_progress_stories/total_stories*100:.0f}% if total_stories > 0 else 0)
- ‚ùå Incomplete: {incomplete_stories} ({incomplete_stories/total_stories*100:.0f}% if total_stories > 0 else 0)

### By Type
- üéØ Features: {len(features)}
- üêõ Bug Fixes: {len(bugs)}
- ‚öôÔ∏è Improvements: {len(improvements)}

### By Points
"""

# Add distribution by story points
point_distribution = {}
for story in stories:
    pts = story['points']
    if pts not in point_distribution:
        point_distribution[pts] = {'total': 0, 'completed': 0}
    point_distribution[pts]['total'] += 1
    if story['status'] == 'complete':
        point_distribution[pts]['completed'] += 1

for points in sorted(point_distribution.keys()):
    dist = point_distribution[points]
    metrics_content += f"- {points} points: {dist['completed']}/{dist['total']} stories\n"

metrics_content += f"""

---

## Trends (3-Sprint Rolling Average)

_To be populated with historical data:_

| Metric | Sprint {str(int(sprint_id)-2).zfill(2)} | Sprint {str(int(sprint_id)-1).zfill(2)} | Sprint {sprint_id} | Trend |
|--------|---------|---------|---------|-------|
| Velocity | [TBD] | [TBD] | {completed_points} | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |
| Completion % | [TBD] | [TBD] | {completion_rate:.0f}% | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |
| Coverage % | [TBD] | [TBD] | {total_coverage:.1f}% | [‚ÜóÔ∏è/‚û°Ô∏è/‚ÜòÔ∏è] |

---

## Team Capacity

### Planned vs Actual
```
Planned Capacity:   {total_points} pts
Actual Delivery:    {completed_points} pts
Utilization:        {completion_rate:.0f}%
```

### Time Allocation (Estimated)
```
Development:  [X]%  {'‚ñà' * 12}{'‚ñë' * 8}
Testing:      [X]%  {'‚ñà' * 5}{'‚ñë' * 15}
Code Review:  [X]%  {'‚ñà' * 3}{'‚ñë' * 17}
Meetings:     [X]%  {'‚ñà' * 2}{'‚ñë' * 18}
Bug Fixes:    [X]%  {'‚ñà' * 1}{'‚ñë' * 19}
```

---

## Risk & Health

### Sprint Health Score: [X]/10

**Components**:
- Velocity: {'‚úÖ' if completed_points >= total_points * 0.85 else '‚ö†Ô∏è'} ({completion_rate:.0f}%)
- Quality: ‚úÖ ({total_coverage:.1f}% coverage)
- Team Happiness: [TBD]
- Technical Debt: [TBD]

### Risks Identified
"""

if incomplete_story_list:
    metrics_content += f"- ‚ö†Ô∏è {len(incomplete_story_list)} stories incomplete - capacity planning review needed\n"

metrics_content += f"""

---

## Key Takeaways

### Strengths
1. {f'Maintained high code coverage ({total_coverage:.1f}%)' if total_coverage >= 80 else 'Achieved test pass rate of 100%'}
2. {f'Delivered {completed_stories} stories' if completed_stories > 0 else 'Completed sprint planning and setup'}
3. [To be added from team feedback]

### Areas for Improvement
"""

if completion_rate < 85:
    metrics_content += f"1. Story completion rate ({completion_rate:.0f}%) below target (85%)\n"

if incomplete_story_list:
    metrics_content += f"2. {len(incomplete_story_list)} stories require better estimation or decomposition\n"

metrics_content += """3. [To be added from team feedback]

---

## Appendix

### Detailed Story List

"""

for story in stories:
    status_icon = {'complete': '‚úÖ', 'in_progress': 'üîÑ', 'incomplete': '‚ùå'}.get(story['status'], '‚ö†Ô∏è')
    metrics_content += f"**{story['id']}**: {story['title']} - {status_icon} ({story['points']} pts)\n"

metrics_content += f"""

### Files Changed (Top 20)

"""

for file_info in changed_files[:20]:
    status_map = {'A': '‚ûï', 'M': 'üìù', 'D': '‚ûñ', 'R': 'üîÑ'}
    icon = status_map.get(file_info['status'][0], 'üìÑ')
    metrics_content += f"{icon} `{file_info['path']}`\n"

metrics_content += f"""

---

**Dashboard Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Data Source**: Git repository, Sprint plan, Test results
**Accuracy**: 100% (automated data collection)
"""

# Write metrics dashboard
metrics_path = f'sprints/sprint_{sprint_id}/metrics.md'
with open(metrics_path, 'w') as f:
    f.write(metrics_content)

print(f"‚úÖ Metrics dashboard created: {metrics_path}")
```

**Status**: ‚úÖ Phase 9 Complete

---

## Phase 10: Final Verification Summary

**Purpose**: Summarize all validations and verifications performed.

**Status**: ‚úì Creating final verification summary...

### Step 9.1: Compile Verification Summary

```python
# Create final verification summary
final_verification = f"""# Final Closeout Verification - Sprint {sprint_id}

**Sprint**: {sprint_name}
**Closeout Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Status**: ‚úÖ APPROVED FOR CLOSEOUT

---

## Validation Results

### Phase 1: Test Coverage & Pass Rate ‚úÖ
- Test Pass Rate: 100% ‚úÖ
- Code Coverage: {total_coverage:.2f}% ‚úÖ (>= 80% required)
- All quality gates passed

### Phase 2: Session Plan Completion ‚úÖ
- Total Stories: {total_stories}
- Completed: {completed_stories}
- Completion Rate: {completion_rate:.1f}%
- Incomplete work: {'Documented' if incomplete_story_list else 'None'}

### Phase 3: Git Operations ‚úÖ
- Files staged: {TOTAL_CHANGES}
- Commit created: {COMMIT_SHORT_HASH}
- Tag created: sprint-{sprint_id}
- Conventional commit format: ‚úÖ

### Phase 4: Session Documentation ‚úÖ
- Current session updated: state/current_session.md
- Next session plan created: state/next_session_plan.md
- Session continuity maintained: ‚úÖ

### Phase 5: Sprint Summary ‚úÖ
- Comprehensive summary created: sprints/sprint_{sprint_id}/sprint_summary.md
- All metrics documented: ‚úÖ
- File changes cataloged: ‚úÖ

### Phase 6: Roadmap Update ‚úÖ
- CLAUDE.md updated: ‚úÖ
- Sprint {sprint_id} marked complete: ‚úÖ
- Current sprint updated to S{next_sprint_id}: ‚úÖ
- Features marked complete: {len(features)}

### Phase 7: Retrospective Template ‚úÖ
- Template created: sprints/sprint_{sprint_id}/retrospective_template.md
- Metrics pre-populated: ‚úÖ
- Ready for team session: ‚úÖ

### Phase 8: Metrics Dashboard ‚úÖ
- Dashboard created: sprints/sprint_{sprint_id}/metrics.md
- All metrics calculated: ‚úÖ
- Trends documented: ‚úÖ

---

## Files Created/Updated

### Created
- ‚úÖ `sprints/sprint_{sprint_id}/verification.md`
- ‚úÖ `sprints/sprint_{sprint_id}/sprint_summary.md`
- ‚úÖ `sprints/sprint_{sprint_id}/retrospective_template.md`
- ‚úÖ `sprints/sprint_{sprint_id}/metrics.md`
- ‚úÖ `state/next_session_plan.md`

### Updated
- ‚úÖ `state/current_session.md`
- ‚úÖ `CLAUDE.md` (roadmap section)

### Git Operations
- ‚úÖ Commit: `{COMMIT_SHORT_HASH}`
- ‚úÖ Tag: `sprint-{sprint_id}`

---

## Quality Assurance

### Definition of Done
- [‚úÖ] Code written and reviewed
- [‚úÖ] Unit tests written and passing
- [‚úÖ] Integration tests passing
- [‚úÖ] Code merged to main branch
- [‚úÖ] Documentation updated
- [‚úÖ] Acceptance criteria validated
- [‚úÖ] No known defects

### Compliance
- [‚úÖ] 80% code coverage requirement met ({total_coverage:.1f}%)
- [‚úÖ] 100% test pass rate achieved
- [‚úÖ] All stories meet Definition of Done
- [‚úÖ] Incomplete work documented (if applicable)

---

## Sign-off

**Sprint {sprint_id} Closeout Approved**: ‚úÖ
**All Validations Passed**: ‚úÖ
**Ready for Next Sprint**: ‚úÖ

**Verified By**: Claude Code `/end` Command
**Verification Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Sprint Tag**: sprint-{sprint_id}

---

This sprint has successfully completed all closeout requirements and is approved.
Next sprint (S{next_sprint_id}) can begin with `/start` command.
"""

print("\n" + "="*80)
print("‚úÖ FINAL VERIFICATION SUMMARY")
print("="*80)
print(final_verification)
print("="*80)
```

**Status**: ‚úÖ Phase 10 Complete

---

## Phase 11: Closeout Summary & Next Steps

**Purpose**: Display comprehensive closeout summary and provide next steps.

**Status**: üéØ Generating closeout summary...

### Step 10.1: Display Comprehensive Summary

```bash
echo ""
echo "=============================================================================="
echo "üéâ SPRINT ${sprint_id} CLOSEOUT COMPLETE!"
echo "=============================================================================="
echo ""
echo "Sprint: ${sprint_name}"
echo "Period: ${sprint_start} - ${sprint_end}"
echo "Closeout: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
echo ""
echo "------------------------------------------------------------------------------"
echo "METRICS SUMMARY"
echo "------------------------------------------------------------------------------"
echo "Velocity:          ${completed_points}/${total_points} points (${completion_rate}%)"
echo "Stories:           ${completed_stories}/${total_stories} completed"
echo "Code Coverage:     ${total_coverage}%"
echo "Test Pass Rate:    100%"
echo "Files Changed:     ${TOTAL_CHANGES}"
echo "Code Changes:      +${INSERTIONS}/-${DELETIONS} lines"
echo ""
echo "------------------------------------------------------------------------------"
echo "ACCOMPLISHMENTS"
echo "------------------------------------------------------------------------------"
```

```python
# Display accomplishments
for story in stories:
    if story['status'] == 'complete':
        print(f"‚úÖ {story['id']}: {story['title']} ({story['points']} pts)")
```

```bash
echo ""
echo "------------------------------------------------------------------------------"
echo "DOCUMENTATION CREATED"
echo "------------------------------------------------------------------------------"
echo "üìÑ Sprint Summary:        sprints/sprint_${sprint_id}/sprint_summary.md"
echo "üìÑ Verification:          sprints/sprint_${sprint_id}/verification.md"
echo "üìÑ Retrospective Template: sprints/sprint_${sprint_id}/retrospective_template.md"
echo "üìÑ Metrics Dashboard:     sprints/sprint_${sprint_id}/metrics.md"
echo "üìÑ Next Session Plan:     state/next_session_plan.md"
echo ""
echo "------------------------------------------------------------------------------"
echo "GIT OPERATIONS"
echo "------------------------------------------------------------------------------"
echo "üì¶ Commit: ${COMMIT_SHORT_HASH}"
echo "üè∑Ô∏è  Tag:    sprint-${sprint_id}"
echo ""
echo "To push to remote:"
echo "  git push origin main --tags"
echo ""
echo "------------------------------------------------------------------------------"
echo "NEXT STEPS"
echo "------------------------------------------------------------------------------"
echo ""
echo "1. üîÑ Conduct Retrospective"
echo "   Open: sprints/sprint_${sprint_id}/retrospective_template.md"
echo "   Complete team retrospective session"
echo ""
echo "2. üìä Review Metrics"
echo "   Open: sprints/sprint_${sprint_id}/metrics.md"
echo "   Analyze sprint performance and trends"
echo ""
echo "3. üì§ Push Changes"
echo "   Command: git push origin main --tags"
echo "   Share sprint closeout with team"
echo ""
echo "4. üìÖ Sprint Review/Demo"
echo "   Schedule sprint review meeting"
echo "   Demo completed features to stakeholders"
echo ""
echo "5. üöÄ Start Next Sprint"
echo "   Command: /start"
echo "   Begin Sprint ${next_sprint_id}"
echo ""
echo "=============================================================================="
echo "Sprint ${sprint_id} closeout successful! üéâ"
echo "=============================================================================="
echo ""
```

**Status**: ‚úÖ Phase 11 Complete

---

## Execution Summary

The `/end` command has completed all 11 phases:

1. ‚úÖ **Test Coverage & Pass Rate Validation** - Verified 80% coverage, 100% pass rate
2. ‚úÖ **Session Plan Completion Validation** - Confirmed all tasks complete or documented
3. ‚úÖ **Modularity Architecture Validation** - Verified modular architecture compliance
4. ‚úÖ **Git Operations** - Staged all files, created commit and tag
5. ‚úÖ **Session Documentation Updates** - Updated current session, created next session plan
6. ‚úÖ **Sprint Summary Creation** - Comprehensive sprint summary with all details
7. ‚úÖ **CLAUDE.md Roadmap Update** - Updated roadmap with sprint completion
8. ‚úÖ **Retrospective Template** - Created pre-populated retrospective
9. ‚úÖ **Metrics Dashboard** - Generated visual metrics dashboard
10. ‚úÖ **Final Verification** - Compiled all verification results
11. ‚úÖ **Closeout Summary** - Displayed results and next steps

### Sprint Closeout Checklist

- [‚úÖ] Tests passing with 80%+ coverage
- [‚úÖ] All session tasks completed or documented
- [‚úÖ] Git commit and tag created
- [‚úÖ] Session documentation updated
- [‚úÖ] Sprint summary created
- [‚úÖ] Roadmap synchronized
- [‚úÖ] Retrospective template ready
- [‚úÖ] Metrics dashboard generated
- [‚úÖ] Verification document created
- [‚úÖ] Next steps provided

### What's Next?

Run `/start` to begin the next sprint with full context from this closeout!
